{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "Torchtext version: 0.8.0a0+0f911ec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import codecs\n",
    "import gensim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\\nTorchtext version: {torchtext.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f90cc05e990>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backend.cudnn.deterministic = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data_path, cv=10):\n",
    "  \n",
    "    with open(os.path.join(data_path, \"rt-polarity.pos\"), \"r\") as f:\n",
    "        pos = f.read()\n",
    "    \n",
    "    with open(os.path.join(data_path, \"rt-polarity.neg\"), \"r\") as f:\n",
    "        neg = f.read()\n",
    "        \n",
    "    pos_sentences = [text_preprocessing(sent) for sent in pos.split('\\n')[:-1]]\n",
    "    neg_sentences = [text_preprocessing(sent) for sent in neg.split('\\n')[:-1]]\n",
    "    \n",
    "    pos_length = len(pos_sentences)\n",
    "    neg_length = len(neg_sentences)\n",
    "    tot_length = pos_length + neg_length\n",
    "    \n",
    "    print(f'Positive: {pos_length}\\nNegative: {neg_length}\\nTotal: {tot_length}')\n",
    "    \n",
    "    reviews = [0] * tot_length\n",
    "\n",
    "    for i, sent in enumerate(pos_sentences):\n",
    "        datum = {'label': 1,\n",
    "                 'text': sent}\n",
    "        reviews[i] = datum\n",
    "\n",
    "    for j, sent in enumerate(neg_sentences):\n",
    "        datum = {'label': 0,\n",
    "                 'text': sent}\n",
    "        reviews[pos_length + j] = datum\n",
    "\n",
    "    word_to_idx = {'@pad': 0}\n",
    "\n",
    "    for sentence in pos_sentences + neg_sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in word_to_idx:\n",
    "                word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "    print(f'number of reviews: {len(reviews)}\\nnumber of vocabularies: {len(word_to_idx)}')\n",
    "    \n",
    "    result_fname = os.path.join(data_path, 'polarity_df.csv')\n",
    "    if not os.path.isfile(result_fname):\n",
    "        df = pd.DataFrame(reviews) \n",
    "        df.to_csv(os.path.join(data_path, 'polarity_df.csv'), index=False, encoding='utf-8')\n",
    "        print('reviews was successfully saved!')\n",
    "    else:\n",
    "        print('reviews was already exist!')\n",
    "    \n",
    "    return reviews, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 5331\n",
      "Negative: 5331\n",
      "Total: 10662\n",
      "number of reviews: 10662\n",
      "number of vocabularies: 18765\n",
      "reviews was already exist!\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data\"\n",
    "revs, word_to_idx = build_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 9596\n",
      "test data: 1066\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT   = torchtext.data.Field(sequential=True, tokenize=str.split, batch_first=True, fix_length=56, lower=True)\n",
    "LABEL  = torchtext.data.LabelField(sequential=False, dtype=torch.float)\n",
    "FIELDS = [('label', LABEL), ('text', TEXT)]\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(os.path.join(data_path, \"polarity_df.csv\"), fields=FIELDS, format='csv', skip_header=True)\n",
    "\n",
    "train_data, test_data = dataset.split(random_state=random.seed(SEED), split_ratio=0.9)\n",
    "\n",
    "print(f'train_data: {len(train_data)}\\ntest data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(data_path, 'GoogleNews-vectors-negative300.bin.gz'), binary = True)\n",
    "word2vec_index = {token: token_index for token_index, token in enumerate(word2vec.index2word)}\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=20000)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "TEXT.vocab.set_vectors(word2vec_index, torch.from_numpy(word2vec.vectors).float().to(device), 300)\n",
    "# TEXT.vocab.vectors.shape\n",
    "# pretrained_embeddings = torch.FloatTensor(TEXT.vocab.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = torch.FloatTensor(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making iterators\n",
    "train_iterator,  test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device, \n",
    "    sort=False, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx, freeze_mode):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels=embedding_dim,\n",
    "                                              out_channels=n_filters,\n",
    "                                              kernel_size=fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        #in: [batch size, sent len]\n",
    "        embedded = self.embedding(text) \n",
    "        #out: [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        #out: [batch size, emb dim, sent len]\n",
    "\n",
    "        conved = [F.tanh(conv(embedded)) for conv in self.convs]\n",
    "        #out: [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #out: [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #out: [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "# VOCAB_SIZE = len(TEXT.vocab)\n",
    "PRETRAINED_EMBEDDINGS = pretrained_embeddings\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "FREEZE_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM 17863\n",
      "EMBEDDING_DIM 300\n",
      "PAD_IDX 1\n",
      "UNK_IDX 0\n"
     ]
    }
   ],
   "source": [
    "print('INPUT_DIM', INPUT_DIM)\n",
    "print('EMBEDDING_DIM', EMBEDDING_DIM)\n",
    "print('PAD_IDX', PAD_IDX)\n",
    "print('UNK_IDX', UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,719,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = CNN1d(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, \n",
    "              FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX, FREEZE_MODE)\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.nn.init.uniform_(torch.empty(EMBEDDING_DIM), -0.25, 0.25)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "model.embedding.weight.requires_grad = True\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "\n",
    "# BCEWithLogitsLoss automatically does softmax function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model     = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_scailing(model, max_val=3, eps=1e-8):\n",
    "    param = model.fc.weight.norm()\n",
    "    norm = param.norm(2, dim=0, keepdim=True)\n",
    "    # torch.cla\n",
    "    desired = torch.clamp(norm, 0, max_val)\n",
    "    param = param * (desired / (eps + norm))\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if True:\n",
    "            max_norm_scailing(model, max_val=3)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.568 | Train Acc: 70.61%\n",
      "\tTest Loss: 0.480 |  Val. Acc: 76.58%\n",
      "Epoch: 02 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.429 | Train Acc: 80.31%\n",
      "\tTest Loss: 0.439 |  Val. Acc: 80.74%\n",
      "Epoch: 03 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.343 | Train Acc: 85.63%\n",
      "\tTest Loss: 0.426 |  Val. Acc: 80.41%\n",
      "Epoch: 04 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.61%\n",
      "\tTest Loss: 0.551 |  Val. Acc: 74.15%\n",
      "Epoch: 05 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.198 | Train Acc: 92.97%\n",
      "\tTest Loss: 0.515 |  Val. Acc: 78.07%\n",
      "Epoch: 06 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.125 | Train Acc: 96.10%\n",
      "\tTest Loss: 0.576 |  Val. Acc: 78.05%\n",
      "Epoch: 07 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.076 | Train Acc: 97.82%\n",
      "\tTest Loss: 0.534 |  Val. Acc: 81.01%\n",
      "Epoch: 08 | Epoch Time: 0m 27s\n",
      "\tTrain Loss: 0.043 | Train Acc: 99.11%\n",
      "\tTest Loss: 0.578 |  Val. Acc: 80.32%\n",
      "Epoch: 09 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.45%\n",
      "\tTest Loss: 0.620 |  Val. Acc: 79.71%\n",
      "Epoch: 10 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.74%\n",
      "\tTest Loss: 0.660 |  Val. Acc: 79.95%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), os.path.join('weights', 'latest_weigths.pt'))\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} |  Val. Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.427 | Test Acc: 80.17%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join('weights', 'latest_weigths.pt')))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted lable: Neg\n",
      "probability: 0.05411254242062569\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence, model, fixed_length=56):\n",
    "    word2id = []\n",
    "\n",
    "    for word in my_sentence.split():\n",
    "        word2id.append(TEXT.vocab.stoi[word])\n",
    "        \n",
    "    word2id = word2id + [1] * (fixed_length - len(word2id))\n",
    "    input_tensor = torch.LongTensor(word2id).to(device).unsqueeze(0)\n",
    "    probability = np.squeeze(torch.sigmoid(model(input_tensor)).detach().numpy()[0], 0)\n",
    "    predicted_label = 'Pos' if probability >= 0.5 else 'Neg' \n",
    "    \n",
    "    return probability, predicted_label\n",
    "\n",
    "my_sentence = \"this film is terrible\"\n",
    "probability, predicted_label = predict(my_sentence, model)\n",
    "\n",
    "print(f\"predicted lable: {predicted_label}\\nprobability: {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
