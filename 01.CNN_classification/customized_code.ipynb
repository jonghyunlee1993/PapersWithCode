{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "Torchtext version: 0.8.0a0+0f911ec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import codecs\n",
    "import gensim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\\nTorchtext version: {torchtext.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdd5685d9b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backend.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(data_path, cv=10):\n",
    "  \n",
    "    revs = []\n",
    "    pos = codecs.open(os.path.join(data_path, \"rt-polarity.pos\"), \"r\", encoding='utf-8', errors='ignore').read()\n",
    "    neg = codecs.open(os.path.join(data_path, \"rt-polarity.neg\"), \"r\", encoding='utf-8', errors='ignore').read()\n",
    "    pos_list = [clean_str(sent) for sent in pos.split('\\n')[:-1]]\n",
    "    neg_list = [clean_str(sent) for sent in neg.split('\\n')[:-1]]\n",
    "    print('pos len', len(pos_list))\n",
    "    print('neg len', len(neg_list))\n",
    "    print('total len', len(pos_list) + len(neg_list) )\n",
    "  \n",
    "    for sent in pos_list:\n",
    "        datum = {'label': 1,\n",
    "                 'text': sent,\n",
    "                 'num_words': len(sent.split()),\n",
    "                 'split': np.random.randint(0,cv)\n",
    "                 }\n",
    "        revs.append(datum)\n",
    "\n",
    "    for sent in neg_list:\n",
    "        datum = {'label': 0,\n",
    "                 'text': sent,\n",
    "                 'num_words': len(sent.split()),\n",
    "                 'split': np.random.randint(0,cv)\n",
    "                }\n",
    "        revs.append(datum)\n",
    "\n",
    "    word_to_idx = {'@pad': 0}\n",
    "\n",
    "    for sent in pos_list + neg_list:\n",
    "        for word  in sent.split():\n",
    "            if word not in word_to_idx:\n",
    "                word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "    print(f'num of total data: {len(revs)}')\n",
    "    print(f'num of vocab: {len(word_to_idx)}')\n",
    "\n",
    "    df = pd.DataFrame(revs) \n",
    "    df.to_csv(os.path.join(data_path, 'polarity_df.csv'), index=False, encoding='utf-8')\n",
    "    \n",
    "    print('save the data')\n",
    "    \n",
    "    return revs, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos len 5331\n",
      "neg len 5331\n",
      "total len 10662\n",
      "num of total data: 10662\n",
      "num of vocab: 18765\n",
      "save the data\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data\"\n",
    "revs, word_to_idx = build_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data 9596\n",
      "test_data 1066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT   = torchtext.data.Field(sequential=True, tokenize=str.split, batch_first=True, fix_length=56, lower=True)\n",
    "LABEL  = torchtext.data.LabelField(sequential=False, dtype=torch.float)\n",
    "FIELDS = [('label', LABEL), ('text', TEXT)]\n",
    "\n",
    "dataset = torchtext.data.TabularDataset(os.path.join(data_path, \"polarity_df.csv\"), fields=FIELDS, format='csv', skip_header=True)\n",
    "\n",
    "train_data, test_data = dataset.split(random_state=random.seed(SEED), split_ratio=0.9)\n",
    "\n",
    "print('train_data', len(train_data))\n",
    "print('test_data', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fdd5bc806d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(data_path, 'GoogleNews-vectors-negative300.bin.gz'), binary = True)\n",
    "w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e946421f4fc14e75918a4bf450dd8a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370126481/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "w2v_vectors = []\n",
    "\n",
    "for token, idx in tqdm(TEXT.vocab.stoi.items()):\n",
    "    if token in w2v.wv.vocab.keys():\n",
    "        w2v_vectors.append(torch.FloatTensor(w2v[token]))\n",
    "    else:\n",
    "        w2v_vectors.append(torch.zeros(EMBEDDING_DIM))\n",
    "#         w2v_vectors.append(torch.distributions.Uniform(-0.25, +0.25).sample((EMBEDDING_DIM, )))\n",
    "\n",
    "print(len(w2v_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# making iterators\n",
    "train_iterator,  test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device, \n",
    "    sort=False, \n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  741,     3,  4650,  ...,     1,     1,     1],\n",
      "        [   42,    38,    56,  ...,     1,     1,     1],\n",
      "        [ 2735,   982,     8,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    4,  4401,  8082,  ...,     1,     1,     1],\n",
      "        [   10,  2667, 13000,  ...,     1,     1,     1],\n",
      "        [   30,     2,  1602,  ...,     1,     1,     1]])\n",
      "torch.Size([64, 56])\n",
      "\n",
      "tensor([0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0.])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for a , b in train_iterator:\n",
    "    print(a)\n",
    "    print(a.size())\n",
    "    print()\n",
    "    print(b)\n",
    "    print(b.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.static_embedding    = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx, freeze=False)\n",
    "        self.nonstatic_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx, freeze=True)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels=embedding_dim,\n",
    "                                              out_channels=n_filters,\n",
    "                                              kernel_size=fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "\n",
    "        conved = [F.tanh(conv(embedded)) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM 17863\n",
      "EMBEDDING_DIM 300\n",
      "PAD_IDX 1\n",
      "UNK_IDX 0\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "print('INPUT_DIM', INPUT_DIM)\n",
    "print('EMBEDDING_DIM', EMBEDDING_DIM)\n",
    "print('PAD_IDX', PAD_IDX)\n",
    "print('UNK_IDX', UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,719,501 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test string: good\n",
      "test string: 54\n",
      "\n",
      "before apply\n",
      "origianl: [ 0.04052734  0.0625     -0.01745605]\n",
      "torch_vector: tensor([ 0.0853, -0.9901, -3.2475], grad_fn=<SliceBackward>)\n",
      "\n",
      "after apply\n",
      "origianl: [ 0.04052734  0.0625     -0.01745605]\n",
      "torch_vector: tensor([ 0.0405,  0.0625, -0.0175], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# for check up (before applying word2vec)\n",
    "test = 'good'\n",
    "test_indx = TEXT.vocab.stoi[test]\n",
    "print(f'test string: {test}\\ntest string: {test_indx}')\n",
    "\n",
    "original_vector = w2v[test]\n",
    "torch_model_vector = model.embedding(torch.tensor([test_indx]))[0]\n",
    "print()\n",
    "print(f'before apply\\norigianl: {original_vector[:3]}\\ntorch_vector: {torch_model_vector[:3]}')\n",
    "\n",
    "# apply w2v\n",
    "TEXT.vocab.set_vectors(TEXT.vocab.stoi, w2v_vectors, EMBEDDING_DIM)\n",
    "pretrained_embeddings = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# for check up (after applying word2vec)\n",
    "torch_model_vector = model.embedding(torch.tensor([test_indx]))[0]\n",
    "print()\n",
    "print(f'after apply\\norigianl: {original_vector[:3]}\\ntorch_vector: {torch_model_vector[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# padding -> zero vectors\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX]\n",
    "\n",
    "# unknown token -> randomly initialized with uniform distribution\n",
    "model.embedding.weight.data[UNK_IDX] = torch.distributions.Uniform(-0.25, +0.25).sample((EMBEDDING_DIM,))\n",
    "model.embedding.weight.data[UNK_IDX]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "\n",
    "# BCEWithLogitsLoss automatically does softmax function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model     = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #  l2 norm (weight contraints): 3\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param.clamp_(min=-3, max=3)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonghyunlee/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.566 | Train Acc: 70.64%\n",
      "\t Test. Loss: 0.494 |  Val. Acc: 75.38%\n",
      "Epoch: 02 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.428 | Train Acc: 79.84%\n",
      "\t Test. Loss: 0.437 |  Val. Acc: 79.94%\n",
      "Epoch: 03 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.342 | Train Acc: 85.41%\n",
      "\t Test. Loss: 0.431 |  Val. Acc: 80.64%\n",
      "Epoch: 04 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.266 | Train Acc: 89.24%\n",
      "\t Test. Loss: 0.434 |  Val. Acc: 79.67%\n",
      "Epoch: 05 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.191 | Train Acc: 92.96%\n",
      "\t Test. Loss: 0.463 |  Val. Acc: 80.03%\n",
      "Epoch: 06 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.124 | Train Acc: 95.75%\n",
      "\t Test. Loss: 0.506 |  Val. Acc: 80.04%\n",
      "Epoch: 07 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.071 | Train Acc: 98.11%\n",
      "\t Test. Loss: 0.541 |  Val. Acc: 81.00%\n",
      "Epoch: 08 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.89%\n",
      "\t Test. Loss: 0.592 |  Val. Acc: 81.41%\n",
      "Epoch: 09 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.61%\n",
      "\t Test. Loss: 0.616 |  Val. Acc: 80.73%\n",
      "Epoch: 10 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.91%\n",
      "\t Test. Loss: 0.660 |  Val. Acc: 80.55%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), os.path.join('weights', 'latest_weigths.pt'))\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Test. Loss: {test_loss:.3f} |  Val. Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.428 | Test Acc: 80.73%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join('weights', 'latest_weigths.pt')))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted lable: Neg\n",
      "probability: 0.057798054069280624\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence, model, fixed_length=56):\n",
    "    word2id = []\n",
    "\n",
    "    for word in my_sentence.split():\n",
    "        word2id.append(TEXT.vocab.stoi[word])\n",
    "        \n",
    "    word2id = word2id + [1] * (fixed_length - len(word2id))\n",
    "    input_tensor = torch.LongTensor(word2id).to(device).unsqueeze(0)\n",
    "    probability = np.squeeze(torch.sigmoid(model(input_tensor)).detach().numpy()[0], 0)\n",
    "    predicted_label = 'Pos' if probability >= 0.5 else 'Neg' \n",
    "    \n",
    "    return probability, predicted_label\n",
    "\n",
    "my_sentence = \"this film is terrible\"\n",
    "probability, predicted_label = predict(my_sentence, model)\n",
    "\n",
    "print(f\"predicted lable: {predicted_label}\\nprobability: {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
