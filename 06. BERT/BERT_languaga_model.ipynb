{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc8dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fpath='data/tokenizer_model'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(fpath,\n",
    "                                              strip_accents=False,\n",
    "                                              lowercase=False)\n",
    "\n",
    "vocab_dim     = len(tokenizer.vocab)\n",
    "seq_len       = 256\n",
    "embedding_dim = 512\n",
    "device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size    = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82d6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLangaugeModelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=128, masking_rate=0.15, NSP_rate=0.5):\n",
    "        super(BERTLangaugeModelDataset, self).__init__()\n",
    "\n",
    "        self.data         = data        \n",
    "        self.tokenizer    = tokenizer\n",
    "        self.vocab        = tokenizer.vocab\n",
    "        self.seq_len      = seq_len\n",
    "        self.masking_rate = masking_rate\n",
    "        self.NSP_rate     = NSP_rate\n",
    "        \n",
    "        self.cls_token_id  = self.tokenizer.cls_token_id\n",
    "        self.sep_token_id  = self.tokenizer.sep_token_id\n",
    "        self.pad_token_id  = self.tokenizer.pad_token_id\n",
    "        self.mask_token_id = self.tokenizer.mask_token_id\n",
    "        \n",
    "    def __getitem__(self, sent_1_idx):       \n",
    "        sent_1 = self.tokenizer.encode(self.data[sent_1_idx])[1:-1]\n",
    "        sent_2_idx = sent_1_idx + 1\n",
    "        \n",
    "        # NSP\n",
    "        if torch.rand(1) >= self.NSP_rate and sent_2_idx != len(self.data):\n",
    "            is_next = torch.tensor(1)\n",
    "        else:\n",
    "            while sent_2_idx == sent_1_idx + 1:\n",
    "                sent_2_idx = torch.randint(0, len(self.data), (1,))\n",
    "            is_next = torch.tensor(0)\n",
    "\n",
    "        sent_2 = self.tokenizer.encode(self.data[sent_2_idx])[1:-1]\n",
    "        \n",
    "        # if length of (sent 1 + sent 2) longer than threshold\n",
    "        # CLS, SEP 1 and 2\n",
    "        if len(sent_1) + len(sent_2) >= self.seq_len - 3:\n",
    "            if len(sent_1) >= self.seq_len -3:\n",
    "                sent_1 = sent_1[:int(self.seq_len/2)]\n",
    "                \n",
    "            sent_2 = sent_2[:self.seq_len - 3 - len(sent_1)]\n",
    "        \n",
    "        pad_length = self.seq_len - 3 - len(sent_1) - len(sent_2)\n",
    "        target = torch.tensor([self.cls_token_id] + sent_1 + [self.sep_token_id] + sent_2 + [self.sep_token_id] + [self.pad_token_id] * pad_length).long().contiguous()        \n",
    "\n",
    "        sengment_embedding = torch.zeros(target.size(0))\n",
    "        sengment_embedding[(len(sent_1) + 2):] = 1\n",
    "        \n",
    "        # masking\n",
    "        masked_sent_1, masking_label_sent_1 = self.masking(sent_1)\n",
    "        masked_sent_2, masking_label_sent_2 = self.masking(sent_2)\n",
    "        \n",
    "        masking_label = torch.cat([\n",
    "            torch.tensor([self.pad_token_id]),\n",
    "            masking_label_sent_1, \n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            masking_label_sent_2,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            torch.tensor([self.pad_token_id] * pad_length)\n",
    "        ])\n",
    "        \n",
    "        # MLM\n",
    "        train = torch.cat([\n",
    "            torch.tensor([self.cls_token_id]), \n",
    "            masked_sent_1,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            masked_sent_2,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            torch.tensor([self.pad_token_id] * pad_length)\n",
    "        ]).long().contiguous()\n",
    "        \n",
    "        return train, target, sengment_embedding, is_next, masking_label\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x\n",
    "            \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.tokenizer.batch_decode(x)\n",
    "    \n",
    "    \n",
    "    # TODO mask 안에서 random 으로 바꿔주는 것 추가\n",
    "    def masking(self, x):\n",
    "        x = torch.tensor(x).long().contiguous()\n",
    "        masking_idx   = torch.randperm(x.size()[0])[:round(x.size()[0] * self.masking_rate) + 1]\n",
    "        masking_label = torch.zeros(x.size()[0])\n",
    "        masking_label[masking_idx] = 1\n",
    "        x = x.masked_fill(masking_label.bool(), self.mask_token_id)\n",
    "        \n",
    "        return x, masking_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f57e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inhyeokyoo.github.io/project/nlp/bert-issue/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, pad_token_id):\n",
    "        super(BERT, self).__init__()\n",
    "        self.pad_token_id  = pad_token_id\n",
    "        self.nhead         = 8\n",
    "        self.embedding     = BERTEmbedding(vocab_dim, seq_len, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=self.nhead, batch_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        pad_mask  = BERT.get_attn_pad_mask(data, data, self.pad_token_id).repeat(self.nhead, 1, 1)\n",
    "        embedding = self.embedding(data, segment_embedding)\n",
    "        output    = self.encoder_block(embedding, pad_mask) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "        batch_size, len_q = seq_q.size()\n",
    "        batch_size, len_k = seq_k.size()\n",
    "        pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "        pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "        \n",
    "        return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa52cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, dropout_rate=0.1, device=device):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.seq_len       = seq_len\n",
    "        self.vocab_dim     = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate  = dropout_rate\n",
    "        \n",
    "        # vocab --> embedding\n",
    "        self.token_embedding      = nn.Embedding(self.vocab_dim, self.embedding_dim) \n",
    "        self.token_dropout        = nn.Dropout(self.dropout_rate)    \n",
    "        \n",
    "        # seq len --> embedding\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.embedding_dim)\n",
    "        self.positional_dropout   = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        # segment (0, 1) --> embedding\n",
    "        self.segment_embedding    = nn.Embedding(2, self.embedding_dim)\n",
    "        self.segment_dropout      = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        token_embedding      = self.token_embedding(data)\n",
    "        token_embedding      = self.token_dropout(token_embedding)\n",
    "        \n",
    "        positional_encoding  = torch.arange(start=0, end=self.seq_len, step=1).long()\n",
    "        # data의 device 정보 가져와서 처리\n",
    "        positional_encoding  = positional_encoding.unsqueeze(0).expand(data.size()).to(device)\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        positional_embedding = self.positional_dropout(positional_embedding)\n",
    "        \n",
    "        segment_embedding    = self.segment_embedding(segment_embedding)\n",
    "        segment_embedding    = self.segment_dropout(segment_embedding)\n",
    "        \n",
    "        return token_embedding + positional_embedding + segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69529541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModeling(nn.Module):\n",
    "    def __init__(self, bert, output_dim):\n",
    "        super(MaskedLanguageModeling, self).__init__()\n",
    "        self.bert = bert\n",
    "        d_model   = bert.embedding.token_embedding.weight.size(1)\n",
    "        self.fc   = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, segment_embedding):\n",
    "        output = self.bert(x, segment_embedding)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class NextSentencePrediction(nn.Module):\n",
    "    def __init__(self, bert, output_dim=2):\n",
    "        super(NextSentencePrediction, self).__init__()\n",
    "        self.bert = bert\n",
    "        d_model   = bert.embedding.token_embedding.weight.size(1)\n",
    "        self.fc   = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x, segment_embedding):\n",
    "        output = self.bert(x, segment_embedding)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output[:, 0, :] # CLS token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceeb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mlm_head, nsp_head, iterator, optimizer, criterion, device, clip=1):\n",
    "    mlm_head.train()\n",
    "    nsp_head.train()\n",
    "    \n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    \n",
    "    for X, y_mlm, segment_emb, y_nsp, masking_label in tqdm(iterator, total=len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mlm_output = mlm_head(X.to(device), segment_emb.long().to(device))     \n",
    "        output_dim = mlm_output.shape[-1]\n",
    "        \n",
    "        #         mlm_output = mlm_output.reshape(-1, mlm_output.shape[-1])\n",
    "        #         mlm_loss   = criterion(mlm_output, y_mlm.to(device).reshape(-1)) # CE\n",
    "        \n",
    "        mlm_output = mlm_output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "        mlm_target = y_mlm.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "        \n",
    "        mlm_loss   = criterion(mlm_output, mlm_target)\n",
    "        \n",
    "        nsp_output = nsp_head(X.to(device), segment_emb.long().to(device))\n",
    "        nsp_loss   = criterion(nsp_output, y_nsp.to(device)) # no need for reshape target\n",
    "        \n",
    "        loss = mlm_loss + nsp_loss\n",
    "        loss.backward()\n",
    "                \n",
    "        torch.nn.utils.clip_grad_norm_(mlm_head.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(nsp_head.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        mlm_epoch_loss += mlm_loss.item()\n",
    "        nsp_epoch_loss += nsp_loss.item()\n",
    "\n",
    "    return mlm_epoch_loss / len(iterator), nsp_epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mlm_head, nsp_head, iterator, optimizer, criterion, device, clip=1):\n",
    "    mlm_head.eval()\n",
    "    nsp_head.eval()\n",
    "    \n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    \n",
    "    for X, y_mlm, segment_emb, y_nsp, masking_label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mlm_output = mlm_head(X.to(device), segment_emb.long().to(device))\n",
    "        output_dim = mlm_output.shape[-1]\n",
    "        \n",
    "        #         mlm_output = mlm_output.reshape(-1, mlm_output.shape[-1])\n",
    "        #         mlm_loss   = criterion(mlm_output, y_mlm.to(device).reshape(-1)) # CE\n",
    "        \n",
    "        mlm_output = mlm_output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "        mlm_target = y_mlm.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "        \n",
    "        mlm_loss   = criterion(mlm_output, mlm_target)\n",
    "        \n",
    "        nsp_output = nsp_head(X.to(device), segment_emb.long().to(device))\n",
    "        nsp_loss   = criterion(nsp_output, y_nsp.to(device)) # no need for reshape target\n",
    "        \n",
    "        mlm_epoch_loss += mlm_loss.item()\n",
    "        nsp_epoch_loss += nsp_loss.item()\n",
    "\n",
    "    return mlm_epoch_loss / len(iterator), nsp_epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(mlm_head, iterator, device, tokenizer):\n",
    "    mlm_head.eval()\n",
    "    \n",
    "    for X, y_mlm, segment_emb, y_nsp, masking_label in iterator:\n",
    "        output = mlm_head(X.to(device), segment_emb.long().to(device))\n",
    "    \n",
    "        target_ = y_mlm.clone().detach().to(\"cpu\")\n",
    "        output_ = torch.argmax(output.clone().detach().to(\"cpu\"), axis=-1)\n",
    "        \n",
    "        target_decode = tokenizer.batch_decode(target_)\n",
    "        output_decode = tokenizer.batch_decode(output_)\n",
    "\n",
    "    return target_decode, output_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e540be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch_prediction_dataloader(data, seq_len, tokenizer, masking_rate, batch_size, collate_fn, shuffle=True, num_workers=5):    \n",
    "    dataset    = BERTLangaugeModelDataset(data=data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=masking_rate)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    \n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd0d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/petitions.txt\", 'r') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "proced_data = [line.replace(\"\\n\", \"\") for line in data]\n",
    "\n",
    "train_data = proced_data[int(len(proced_data) * 0.2):]\n",
    "test_data  = proced_data[:int(len(proced_data) * 0.2)]\n",
    "\n",
    "train_dataset = BERTLangaugeModelDataset(data=train_data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=0.4)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "valid_dataset = BERTLangaugeModelDataset(data=test_data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=0.3)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4007ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adamw.py:47: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(AdamW, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "bert      = BERT(vocab_dim=vocab_dim, seq_len=seq_len, embedding_dim=embedding_dim, pad_token_id=0).to(device)\n",
    "mlm_head  = MaskedLanguageModeling(bert, output_dim=vocab_dim).to(device)\n",
    "nsp_head  = NextSentencePrediction(bert).to(device)\n",
    "optimizer = optim.AdamW(list(mlm_head.parameters()) + list(nsp_head.parameters()), lr=1e-4, betas=[0.9, 0.999], weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786f02e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d95593679ac4938803dedd953c7bc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Train MLM Loss: 6.8506 | Train NSP Loss: 0.7150\n",
      "Valid MLM Loss: 6.6898 | Valid NSP Loss: 0.6940\n",
      "Predictions ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fa4ae20ba74193b5fedc1f83df31f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if len(glob.glob(\"output/*.tsv\")) != 0:\n",
    "    print(\"load pretrained model ... \")\n",
    "    start_epoch = len(glob.glob(\"output/*.tsv\"))\n",
    "    checkpoint = torch.load('weights/BERT_LM_best.pt')\n",
    "    mlm_head.load_state_dict(checkpoint['mlm_head'])\n",
    "    nsp_head.load_state_dict(checkpoint['nsp_head'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "N_EPOCHS  = 1000\n",
    "PAITIENCE = 30\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    train_mlm_loss, train_nsp_loss = train(mlm_head, nsp_head, train_dataloader, optimizer, criterion, device)\n",
    "    valid_mlm_loss, valid_nsp_loss = evaluate(mlm_head, nsp_head, valid_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    valid_loss = valid_mlm_loss + valid_nsp_loss\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:04}')\n",
    "    print(f'Train MLM Loss: {train_mlm_loss:.4f} | Train NSP Loss: {train_nsp_loss:.4f}')\n",
    "    print(f'Valid MLM Loss: {valid_mlm_loss:.4f} | Valid NSP Loss: {valid_nsp_loss:.4f}')\n",
    "    \n",
    "    with open(\"output/log.txt\", \"a\") as f:\n",
    "        f.write(\"epoch: {0:04d} train mlm loss: {1:.4f}, train nsp loss: {2:.4f}, valid mlm loss: {3:.4f}, valid nsp loss: {4:.4f} \\n\".format(epoch, train_mlm_loss, train_nsp_loss, valid_mlm_loss, valid_nsp_loss))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Predictions ...\\n\")\n",
    "        sampled_for_prediction = shuffle(test_data, n_samples=20)\n",
    "        prediction_dataloader  = generate_epoch_prediction_dataloader(\n",
    "                                                                        sampled_for_prediction, \n",
    "                                                                        seq_len=seq_len, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        batch_size=len(sampled_for_prediction), \n",
    "                                                                        masking_rate=0.5, \n",
    "                                                                        collate_fn=collate_fn\n",
    "                                                                        )\n",
    "        output_list, target_list = predict(mlm_head, prediction_dataloader, device, tokenizer)\n",
    "        prediction_results = pd.DataFrame({\"output\": output_list, \"target\": target_list})\n",
    "        prediction_results.to_csv(\"output/prediction_results_epoch-{0:04d}.tsv\".format(epoch), sep=\"\\t\", index=False)            \n",
    "        \n",
    "    \n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                'mlm_head' : mlm_head.state_dict(),\n",
    "                'nsp_head' : nsp_head.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "                }, 'weights/BERT_LM_best.pt'\n",
    "            )\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        checkpoint = torch.load('weights/BERT_LM_best.pt')\n",
    "        mlm_head.load_state_dict(checkpoint['mlm_head'])\n",
    "        nsp_head.load_state_dict(checkpoint['nsp_head'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1859ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
