{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a506f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/GyuminJack/torchstudy/blob/main/06Jun/NER/src/make_ner_data.py\n",
    "\n",
    "import os\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "def klue_to_text_and_bio(path):\n",
    "    corpus_text = []\n",
    "    corpus_bio = []\n",
    "    with open(path, 'r') as f:\n",
    "        _tokens = []\n",
    "        _bio = []\n",
    "        for cnt, line in enumerate(f.readlines()):\n",
    "            if (\"##\" not in line) and (line != \"\\n\"):\n",
    "                line = line.replace(\"\\n\", \"\")\n",
    "                token, bio = line.split(\"\\t\")\n",
    "                _tokens.append(token)\n",
    "                _bio.append(bio)\n",
    "            \n",
    "            elif line == \"\\n\":\n",
    "                assert len(_tokens) == len(_bio), \"Size Mismatched\"\n",
    "                corpus_text.append(\"\".join(_tokens))\n",
    "                corpus_bio.append(_bio)\n",
    "                _tokens = []\n",
    "                _bio = []\n",
    "                \n",
    "    return corpus_text, corpus_bio\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    loaded_tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, strip_accents=False, lowercase=False)\n",
    "    return loaded_tokenizer\n",
    "\n",
    "\n",
    "def get_token_labels(tokenizer, text:str, original_bio:list):\n",
    "    cleaned_original_bio = [lbl for txt, lbl in list(zip(text, original_bio)) if txt.strip()]\n",
    "    \n",
    "    tokenized = tokenizer(text, return_offsets_mapping = True)\n",
    "    token_list = tokenized['input_ids'][1:-1]\n",
    "    offset_list = tokenized['offset_mapping'][1:-1]\n",
    "    \n",
    "    start_index = 0\n",
    "    merged_bio = []\n",
    "    for offset in offset_list:\n",
    "        token_length = offset[1] - offset[0]\n",
    "        seleceted_labels = cleaned_original_bio[start_index : start_index+token_length][0] # 가장 첫번째 bio 태그를 태그로 사용\n",
    "        merged_bio.append(seleceted_labels)\n",
    "        start_index += token_length\n",
    "    \n",
    "    assert len(token_list) == len(merged_bio), \"Size Mismatched\"\n",
    "    if len(token_list) != len(merged_bio):\n",
    "        print(\"aDfasdklj;fjas\")\n",
    "    return token_list, merged_bio\n",
    "\n",
    "\n",
    "def save_ner_data(save_path, tokenizer, text:list, bio:list):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        for _text, _bio in zip(text, bio):\n",
    "            _, new_bio = get_token_labels(tokenizer, _text, _bio)\n",
    "            f.write(_text + \"\\t\" + \",\".join(new_bio) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75d3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text, corpus_bio = klue_to_text_and_bio(\"./data/klue-ner-v1/klue-ner-v1_train.tsv\")\n",
    "tokenizer_path =  \"./data/tokenizer_model\"\n",
    "tokenizer_name = tokenizer_path.split(\"/\")[-1]\n",
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "\n",
    "save_path = \"./data/klue_ner_processed.train\"\n",
    "save_ner_data(save_path, tokenizer, corpus_text, corpus_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68528d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
