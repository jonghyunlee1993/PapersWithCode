{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5817c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fpath = './data/tokenizer_model'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(fpath,\n",
    "                                              strip_accents=False,\n",
    "                                              lowercase=False)\n",
    "\n",
    "vocab_dim     = len(tokenizer.vocab)\n",
    "seq_len       = 256\n",
    "embedding_dim = 512\n",
    "device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size    = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b11e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/GyuminJack/torchstudy/blob/main/06Jun/NER/src/data.py\n",
    "\n",
    "import linecache\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    loaded_tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, strip_accents=False, lowercase=False)  # Must be False if cased model  # 로드\n",
    "    return loaded_tokenizer\n",
    "\n",
    "class KlueDataset_NER(Dataset):\n",
    "    def __init__(self, vocab_txt_path, txt_path, *args, **kwargs):\n",
    "        self.tokenizer = load_tokenizer(vocab_txt_path)\n",
    "        self.max_seq_len = 256\n",
    "        self.txt_path = txt_path\n",
    "        \n",
    "        self.cls_token_id  = self.tokenizer.cls_token_id\n",
    "        self.sep_token_id  = self.tokenizer.sep_token_id\n",
    "        self.pad_token_id  = self.tokenizer.pad_token_id\n",
    "        \n",
    "        self.bio_dict = {\n",
    "                        '[PAD]' : 0,\n",
    "                        'B-DT': 1,\n",
    "                        'B-LC': 2,\n",
    "                        'B-OG': 3,\n",
    "                        'B-PS': 4,\n",
    "                        'B-QT': 5,\n",
    "                        'B-TI': 6,\n",
    "                        'I-DT': 7,\n",
    "                        'I-LC': 8,\n",
    "                        'I-OG': 9,\n",
    "                        'I-PS': 10,\n",
    "                        'I-QT': 11,\n",
    "                        'I-TI': 12,\n",
    "                        'O': 13\n",
    "                        }\n",
    "        self.reverse_bio_dict = {v:k for k, v in self.bio_dict.items()}\n",
    "        with open(self.txt_path, \"r\") as f:\n",
    "            self._total_data = len(f.readlines())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_ko = linecache.getline(self.txt_path, idx + 1).strip()\n",
    "        text, bio_string = raw_ko.split(\"\\t\")\n",
    "        bio_tensor = [self.bio_dict[i] for i in bio_string.split(\",\")]\n",
    "        \n",
    "        sent = self.tokenizer.encode(text)[1:-1]\n",
    "        pad_length = self.max_seq_len - 2 - len(sent)\n",
    "        \n",
    "        train = torch.tensor([self.cls_token_id] + sent + [self.sep_token_id] + [self.pad_token_id] * pad_length).long().contiguous()\n",
    "        target = torch.tensor([self.cls_token_id] + bio_tensor + [self.sep_token_id] + [self.pad_token_id] * pad_length).long().contiguous()\n",
    "        \n",
    "        segment_embedding = torch.zeros(target.size(0)).long()\n",
    "        \n",
    "        return train, target, segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8973d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inhyeokyoo.github.io/project/nlp/bert-issue/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, pad_token_id):\n",
    "        super(BERT, self).__init__()\n",
    "        self.pad_token_id  = pad_token_id\n",
    "        self.nhead         = 8\n",
    "        self.embedding     = BERTEmbedding(vocab_dim, seq_len, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=self.nhead, batch_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        pad_mask  = BERT.get_attn_pad_mask(data, data, self.pad_token_id).repeat(self.nhead, 1, 1)\n",
    "        embedding = self.embedding(data, segment_embedding)\n",
    "        output    = self.encoder_block(embedding, pad_mask) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "        batch_size, len_q = seq_q.size()\n",
    "        batch_size, len_k = seq_k.size()\n",
    "        pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "        pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "        \n",
    "        return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8056e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, dropout_rate=0.1, device=device):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.seq_len       = seq_len\n",
    "        self.vocab_dim     = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate  = dropout_rate\n",
    "        \n",
    "        # vocab --> embedding\n",
    "        self.token_embedding      = nn.Embedding(self.vocab_dim, self.embedding_dim) \n",
    "        self.token_dropout        = nn.Dropout(self.dropout_rate)    \n",
    "        \n",
    "        # seq len --> embedding\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.embedding_dim)\n",
    "        self.positional_dropout   = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        # segment (0, 1) --> embedding\n",
    "        self.segment_embedding    = nn.Embedding(2, self.embedding_dim)\n",
    "        self.segment_dropout      = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        token_embedding      = self.token_embedding(data)\n",
    "        token_embedding      = self.token_dropout(token_embedding)\n",
    "        \n",
    "        positional_encoding  = torch.arange(start=0, end=self.seq_len, step=1).long()\n",
    "        # data의 device 정보 가져와서 처리\n",
    "        positional_encoding  = positional_encoding.unsqueeze(0).expand(data.size()).to(device)\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        positional_embedding = self.positional_dropout(positional_embedding)\n",
    "        \n",
    "        segment_embedding    = self.segment_embedding(segment_embedding)\n",
    "        segment_embedding    = self.segment_dropout(segment_embedding)\n",
    "        \n",
    "        return token_embedding + positional_embedding + segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7873260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class BertNER(nn.Module):\n",
    "    def __init__(self, bert, bert_hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.fc = nn.Linear(bert_hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.crf = CRF(num_tags=num_classes, batch_first=True)\n",
    "\n",
    "    def forward(self, source, target, segment_embedding):\n",
    "        source = self.bert(source, segment_embedding)\n",
    "        last_encoder_output = self.dropout(source)\n",
    "        emissions = self.fc(last_encoder_output)\n",
    "        log_likelihood, output = self.crf(emissions, target), self.crf.decode(emissions)\n",
    "        \n",
    "        return log_likelihood, torch.tensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e413630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, device, clip=1):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc  = 0\n",
    "    \n",
    "    for source, target, segment_embedding in tqdm(iterator, total=len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "        segment_embedding = segment_embedding.to(device)\n",
    "\n",
    "        log_likelihood, output = model(source, target, segment_embedding) \n",
    "        \n",
    "        loss = -1 * log_likelihood\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        target = target.to(\"cpu\")\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        non_zero_index =non_zero_index = (target != 0)\n",
    "        target_non_zero = target[non_zero_index]\n",
    "        output_non_zero = output[non_zero_index]\n",
    "\n",
    "        acc = ((target_non_zero==output_non_zero).float().sum() / len(target_non_zero))\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc  += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, optimizer, device, clip=1):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc  = 0\n",
    "    \n",
    "    for source, target, segment_embedding in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "        segment_embedding = segment_embedding.to(device)\n",
    "\n",
    "        log_likelihood, output = model(source, target, segment_embedding) \n",
    "\n",
    "        loss = -1 * log_likelihood\n",
    "        \n",
    "        target = target.to(\"cpu\")\n",
    "        output = output.to('cpu')\n",
    "\n",
    "        non_zero_index =non_zero_index = (target != 0)\n",
    "        target_non_zero = target[non_zero_index]\n",
    "        output_non_zero = output[non_zero_index]\n",
    "\n",
    "        acc = ((target_non_zero==output_non_zero).float().sum() / len(target_non_zero))\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc  += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb012fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_txt_path = \"./data/tokenizer_model\"\n",
    "\n",
    "train_path = \"./data/klue_ner_processed.train\"\n",
    "train_dataset = KlueDataset_NER(vocab_txt_path, train_path)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_path = \"./data/klue_ner_processed.dev\"\n",
    "valid_dataset = KlueDataset_NER(vocab_txt_path, valid_path)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "checkpoint = torch.load('./weights/BERT_LM_best.pt')\n",
    "bert = BERT(vocab_dim=vocab_dim, seq_len=seq_len, embedding_dim=embedding_dim, pad_token_id=0).to(device)\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "\n",
    "ner_head = BertNER(bert, embedding_dim, len(train_dataset.bio_dict)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(ner_head.parameters(), lr = 0.0001, weight_decay = 0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b32e2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822d69c3540c407fa319a9ad89e8bd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/TensorCompare.cpp:255.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Train Loss: 10316.1697 | Train Acc: 0.8016\n",
      "Valid Loss: 2992.3516 | Valid Acc: 0.8855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e54cbdbd1d0436e832a8ac3a32eb81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002\n",
      "Train Loss: 2398.7176 | Train Acc: 0.9062\n",
      "Valid Loss: 2395.5992 | Valid Acc: 0.9075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471a4beb040046a69365a32fda659525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003\n",
      "Train Loss: 1931.0802 | Train Acc: 0.9225\n",
      "Valid Loss: 2126.0312 | Valid Acc: 0.9158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f3df9bbc9f46dc8014ea31cba1c8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004\n",
      "Train Loss: 1649.6522 | Train Acc: 0.9327\n",
      "Valid Loss: 1967.0039 | Valid Acc: 0.9224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6ab6c658a4ad2852fc3dd919500ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005\n",
      "Train Loss: 1454.7549 | Train Acc: 0.9405\n",
      "Valid Loss: 1887.0495 | Valid Acc: 0.9271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67165a4bd848426b9c7b06926ffea34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006\n",
      "Train Loss: 1302.2208 | Train Acc: 0.9464\n",
      "Valid Loss: 1821.0159 | Valid Acc: 0.9295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaaf77728fb4119998eca81eaf93c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007\n",
      "Train Loss: 1177.8361 | Train Acc: 0.9510\n",
      "Valid Loss: 1825.4787 | Valid Acc: 0.9291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be734202a849b28a7df1a4f94d5b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008\n",
      "Train Loss: 1075.4959 | Train Acc: 0.9551\n",
      "Valid Loss: 1776.8044 | Valid Acc: 0.9325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deabe56cf5f4519815d9632f08e028b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009\n",
      "Train Loss: 987.2065 | Train Acc: 0.9585\n",
      "Valid Loss: 1767.2415 | Valid Acc: 0.9348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f0d8044bf642c08a3aef0b52a6dbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010\n",
      "Train Loss: 913.8428 | Train Acc: 0.9613\n",
      "Valid Loss: 1757.3239 | Valid Acc: 0.9366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e24012ce28f43b29839356b9d42b589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011\n",
      "Train Loss: 836.5510 | Train Acc: 0.9643\n",
      "Valid Loss: 1752.3896 | Valid Acc: 0.9374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6412c35bb524f9a876d73dc7a5f66db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0012\n",
      "Train Loss: 776.9342 | Train Acc: 0.9665\n",
      "Valid Loss: 1768.3807 | Valid Acc: 0.9389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6926b7a3e0461ca40d43a13db6a3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0013\n",
      "Train Loss: 724.6691 | Train Acc: 0.9689\n",
      "Valid Loss: 1764.1499 | Valid Acc: 0.9395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227de06223104fb0b98a4e4c188a46a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0014\n",
      "Train Loss: 667.8331 | Train Acc: 0.9710\n",
      "Valid Loss: 1792.0772 | Valid Acc: 0.9400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8b44a8866f4ef4b7002a7c80430a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0015\n",
      "Train Loss: 618.8147 | Train Acc: 0.9732\n",
      "Valid Loss: 1773.0927 | Valid Acc: 0.9403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd0f823c5d34e3ab50fd7e391a4d36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0016\n",
      "Train Loss: 577.9724 | Train Acc: 0.9749\n",
      "Valid Loss: 1835.9813 | Valid Acc: 0.9406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a2f8e21db74209aa461d8d1a3e8831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0017\n",
      "Train Loss: 541.7198 | Train Acc: 0.9763\n",
      "Valid Loss: 1870.0714 | Valid Acc: 0.9399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e751aa394f8048df9d7ecf5d9b26f325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0018\n",
      "Train Loss: 508.3576 | Train Acc: 0.9776\n",
      "Valid Loss: 1847.8342 | Valid Acc: 0.9413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7f686231b49bab410e290bfb55e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "N_EPOCHS  = 1000\n",
    "PAITIENCE = 30\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    train_loss, train_acc = train(ner_head, train_data_loader, optimizer, device)\n",
    "    valid_loss, valid_acc = evaluate(ner_head, valid_data_loader, optimizer, device)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:04}')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "    print(f'Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    with open(\"output/log_ner.txt\", \"a\") as f:\n",
    "        f.write(\"epoch: {0:04d} train loss: {1:.4f}, train acc: {2:.4f}, valid loss: {3:.4f}, valid acc: {4:.4f} \\n\".format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "\n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                'ner_head' : ner_head.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "                }, 'weights/BERT_ner_best.pt'\n",
    "            )\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        checkpoint = torch.load('weights/BERT_ner_best.pt')\n",
    "        ner_head.load_state_dict(checkpoint['ner_head'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f5762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
