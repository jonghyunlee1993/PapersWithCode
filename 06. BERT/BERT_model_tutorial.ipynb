{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc8dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fpath='data/tokenizer_model'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(fpath,\n",
    "                                              strip_accents=False,\n",
    "                                              lowercase=False)\n",
    "\n",
    "vocab_dim     = len(tokenizer.vocab)\n",
    "seq_len       = 256\n",
    "embedding_dim = 512\n",
    "device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device        = \"cpu\"\n",
    "batch_size    = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82d6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLangaugeModelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=128, masking_rate=0.15, NSP_rate=0.5):\n",
    "        super(BERTLangaugeModelDataset, self).__init__()\n",
    "\n",
    "        self.data         = data        \n",
    "        self.tokenizer    = tokenizer\n",
    "        self.vocab        = tokenizer.vocab\n",
    "        self.seq_len      = seq_len\n",
    "        self.masking_rate = masking_rate\n",
    "        self.NSP_rate     = NSP_rate\n",
    "        \n",
    "        self.cls_token_id  = self.tokenizer.cls_token_id\n",
    "        self.sep_token_id  = self.tokenizer.sep_token_id\n",
    "        self.pad_token_id  = self.tokenizer.pad_token_id\n",
    "        self.mask_token_id = self.tokenizer.mask_token_id\n",
    "        \n",
    "    def __getitem__(self, sent_1_idx):       \n",
    "        sent_1 = self.tokenizer.encode(self.data[sent_1_idx])[1:-1]\n",
    "        sent_2_idx = sent_1_idx + 1\n",
    "        \n",
    "        # NSP\n",
    "        if torch.rand(1) >= self.NSP_rate and sent_2_idx != len(self.data):\n",
    "            is_next = torch.tensor(1)\n",
    "        else:\n",
    "            while sent_2_idx == sent_1_idx + 1:\n",
    "                sent_2_idx = torch.randint(0, len(self.data), (1,))\n",
    "            is_next = torch.tensor(0)\n",
    "\n",
    "        sent_2 = self.tokenizer.encode(self.data[sent_2_idx])[1:-1]\n",
    "        \n",
    "        # if length of (sent 1 + sent 2) longer than threshold\n",
    "        # CLS, SEP 1 and 2\n",
    "        if len(sent_1) + len(sent_2) >= self.seq_len - 3:\n",
    "            if len(sent_1) >= self.seq_len -3:\n",
    "                sent_1 = sent_1[:int(self.seq_len/2)]\n",
    "                \n",
    "            sent_2 = sent_2[:self.seq_len - 3 - len(sent_1)]\n",
    "        \n",
    "        pad_length = self.seq_len - 3 - len(sent_1) - len(sent_2)\n",
    "        target = torch.tensor([self.cls_token_id] + sent_1 + [self.sep_token_id] + sent_2 + [self.sep_token_id] + [self.pad_token_id] * pad_length).long().contiguous()        \n",
    "\n",
    "        sengment_embedding = torch.zeros(target.size(0))\n",
    "        sengment_embedding[(len(sent_1) + 2):] = 1\n",
    "        \n",
    "        # masking\n",
    "        masked_sent_1, masking_label_sent_1 = self.masking(sent_1)\n",
    "        masked_sent_2, masking_label_sent_2 = self.masking(sent_2)\n",
    "        \n",
    "        masking_label = torch.cat([\n",
    "            torch.tensor([self.pad_token_id]),\n",
    "            masking_label_sent_1, \n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            masking_label_sent_2,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            torch.tensor([self.pad_token_id] * pad_length)\n",
    "        ])\n",
    "        \n",
    "        # MLM\n",
    "        train = torch.cat([\n",
    "            torch.tensor([self.cls_token_id]), \n",
    "            masked_sent_1,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            masked_sent_2,\n",
    "            torch.tensor([self.sep_token_id]),\n",
    "            torch.tensor([self.pad_token_id] * pad_length)\n",
    "        ]).long().contiguous()\n",
    "        \n",
    "        return train, target, sengment_embedding, is_next, masking_label\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for x in self.data:\n",
    "            yield x\n",
    "            \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.tokenizer.batch_decode(x)\n",
    "    \n",
    "    \n",
    "    # TODO mask 안에서 random 으로 바꿔주는 것 추가\n",
    "    def masking(self, x):\n",
    "        x = torch.tensor(x).long().contiguous()\n",
    "        masking_idx   = torch.randperm(x.size()[0])[:round(x.size()[0] * self.masking_rate) + 1]\n",
    "        masking_label = torch.zeros(x.size()[0])\n",
    "        masking_label[masking_idx] = 1\n",
    "        x = x.masked_fill(masking_label.bool(), self.mask_token_id)\n",
    "        \n",
    "        return x, masking_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f57e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inhyeokyoo.github.io/project/nlp/bert-issue/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, pad_token_id):\n",
    "        super(BERT, self).__init__()\n",
    "        self.pad_token_id  = pad_token_id\n",
    "        self.nhead         = 8\n",
    "        self.embedding     = BERTEmbedding(vocab_dim, seq_len, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=self.nhead, batch_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        pad_mask  = BERT.get_attn_pad_mask(data, data, self.pad_token_id).repeat(self.nhead, 1, 1)\n",
    "        embedding = self.embedding(data, segment_embedding)\n",
    "        output    = self.encoder_block(embedding, pad_mask) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "        batch_size, len_q = seq_q.size()\n",
    "        batch_size, len_k = seq_k.size()\n",
    "        pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "        pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "        \n",
    "        return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa52cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, dropout_rate=0.1, device=device):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.seq_len       = seq_len\n",
    "        self.vocab_dim     = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate  = dropout_rate\n",
    "        \n",
    "        # vocab --> embedding\n",
    "        self.token_embedding      = nn.Embedding(self.vocab_dim, self.embedding_dim) \n",
    "        self.token_dropout        = nn.Dropout(self.dropout_rate)    \n",
    "        \n",
    "        # seq len --> embedding\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.embedding_dim)\n",
    "        self.positional_dropout   = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        # segment (0, 1) --> embedding\n",
    "        self.segment_embedding    = nn.Embedding(2, self.embedding_dim)\n",
    "        self.segment_dropout      = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        token_embedding      = self.token_embedding(data)\n",
    "        token_embedding      = self.token_dropout(token_embedding)\n",
    "        \n",
    "        positional_encoding  = torch.arange(start=0, end=self.seq_len, step=1).long()\n",
    "        # data의 device 정보 가져와서 처리\n",
    "        positional_encoding  = positional_encoding.unsqueeze(0).expand(data.size()).to(device)\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        positional_embedding = self.positional_dropout(positional_embedding)\n",
    "        \n",
    "        segment_embedding    = self.segment_embedding(segment_embedding)\n",
    "        segment_embedding    = self.segment_dropout(segment_embedding)\n",
    "        \n",
    "        return token_embedding + positional_embedding + segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69529541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModeling(nn.Module):\n",
    "    def __init__(self, bert, output_dim):\n",
    "        super(MaskedLanguageModeling, self).__init__()\n",
    "        self.bert = bert\n",
    "        d_model   = bert.embedding.token_embedding.weight.size(1)\n",
    "        self.fc   = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x, segment_embedding):\n",
    "        output = self.bert(x, segment_embedding)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class NextSentencePrediction(nn.Module):\n",
    "    def __init__(self, bert, output_dim=2):\n",
    "        super(NextSentencePrediction, self).__init__()\n",
    "        self.bert = bert\n",
    "        d_model   = bert.embedding.token_embedding.weight.size(1)\n",
    "        self.fc   = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x, segment_embedding):\n",
    "        output = self.bert(x, segment_embedding)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output[:, 0, :] # CLS token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceeb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mlm_head, nsp_head, iterator, optimizer, criterion, device, clip=1):\n",
    "    mlm_head.train()\n",
    "    nsp_head.train()\n",
    "    \n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    \n",
    "    for batch, (X, y_mlm, segment_emb, y_nsp, masking_label) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mlm_output = mlm_head(X.to(device), segment_emb.long().to(device))\n",
    "        print(f\"mlm output : {mlm_output.shape}\")\n",
    "        \n",
    "        output_dim = mlm_output.shape[-1]\n",
    "        \n",
    "        #         mlm_output = mlm_output.reshape(-1, mlm_output.shape[-1])\n",
    "        #         mlm_loss   = criterion(mlm_output, y_mlm.to(device).reshape(-1)) # CE\n",
    "        \n",
    "        print(f\"masking label : {masking_label.shape}\")\n",
    "        print(f\"masking label : {masking_label}\")\n",
    "        \n",
    "        mlm_output = mlm_output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "        print(f\"mlm output after masking : {mlm_output.shape}\")\n",
    "        mlm_target = y_mlm.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "        print(f\"mlm target : {mlm_target.shape}\")\n",
    "        \n",
    "        break\n",
    "        \n",
    "        mlm_loss   = criterion(mlm_output, mlm_target)\n",
    "        \n",
    "        nsp_output = nsp_head(X.to(device), segment_emb.long().to(device))\n",
    "        nsp_loss   = criterion(nsp_output, y_nsp.to(device)) # no need for reshape target\n",
    "        \n",
    "        loss = mlm_loss + nsp_loss\n",
    "        loss.backward()\n",
    "                \n",
    "        torch.nn.utils.clip_grad_norm_(mlm_head.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(nsp_head.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        mlm_epoch_loss += mlm_loss.item()\n",
    "        nsp_epoch_loss += nsp_loss.item()\n",
    "\n",
    "    return mlm_epoch_loss / len(iterator), nsp_epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(mlm_head, nsp_head, iterator, optimizer, criterion, device, clip=1):\n",
    "    mlm_head.eval()\n",
    "    nsp_head.eval()\n",
    "    \n",
    "    mlm_epoch_loss = 0\n",
    "    nsp_epoch_loss = 0\n",
    "    \n",
    "    for batch, (X, y_mlm, segment_emb, y_nsp, masking_label) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mlm_output = mlm_head(X.to(device), segment_emb.long().to(device))\n",
    "        output_dim = mlm_output.shape[-1]\n",
    "        \n",
    "        #         mlm_output = mlm_output.reshape(-1, mlm_output.shape[-1])\n",
    "        #         mlm_loss   = criterion(mlm_output, y_mlm.to(device).reshape(-1)) # CE\n",
    "        \n",
    "        mlm_output = mlm_output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "        mlm_target = y_mlm.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "        \n",
    "        mlm_loss   = criterion(mlm_output, mlm_target)\n",
    "        \n",
    "        nsp_output = nsp_head(X.to(device), segment_emb.long().to(device))\n",
    "        nsp_loss   = criterion(nsp_output, y_nsp.to(device)) # no need for reshape target\n",
    "        \n",
    "        mlm_epoch_loss += mlm_loss.item()\n",
    "        nsp_epoch_loss += nsp_loss.item()\n",
    "\n",
    "    return mlm_epoch_loss / len(iterator), nsp_epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd0d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/petitions.txt\", 'r') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "proced_data = [line.replace(\"\\n\", \"\") for line in data]\n",
    "\n",
    "train_data = proced_data[int(len(proced_data) * 0.2):]\n",
    "test_data  = proced_data[:int(len(proced_data) * 0.2)]\n",
    "\n",
    "train_dataset = BERTLangaugeModelDataset(data=train_data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=0.3)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "valid_dataset = BERTLangaugeModelDataset(data=test_data, seq_len=seq_len, tokenizer=tokenizer, masking_rate=0.3)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4007ad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bert     = BERT(vocab_dim=vocab_dim, seq_len=seq_len, embedding_dim=embedding_dim, pad_token_id=0).to(device)\n",
    "mlm_head = MaskedLanguageModeling(bert, output_dim=vocab_dim).to(device)\n",
    "nsp_head = NextSentencePrediction(bert).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0786f02e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py:48: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea5be111ce742bab2d160b4357574ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm output : torch.Size([128, 256, 32000])\n",
      "masking label : torch.Size([128, 256])\n",
      "masking label : tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "mlm output after masking : torch.Size([3068, 32000])\n",
      "mlm target : torch.Size([3068])\n",
      "Epoch: 0001\n",
      "Train MLM Loss: 0.0000 | Train NSP Loss: 0.0000\n",
      "Valid MLM Loss: 6.6907 | Valid NSP Loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea5a050104643fea0d6739b958fad09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm output : torch.Size([128, 256, 32000])\n",
      "masking label : torch.Size([128, 256])\n",
      "masking label : tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]])\n",
      "mlm output after masking : torch.Size([3068, 32000])\n",
      "mlm target : torch.Size([3068])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-eba528e72504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_mlm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nsp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlm_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsp_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvalid_mlm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_nsp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlm_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsp_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_mlm_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalid_nsp_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-138a7a8b3a75>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(mlm_head, nsp_head, iterator, optimizer, criterion, device, clip)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mnsp_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsp_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mnsp_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsp_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_nsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no need for reshape target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mmlm_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmlm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(list(mlm_head.parameters()) + list(nsp_head.parameters()), lr=1e-4, betas=[0.9, 0.999], weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "N_EPOCHS  = 1000\n",
    "PAITIENCE = 30\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "checkpoint = torch.load('weights/BERT_LM_best.pt')\n",
    "mlm_head.load_state_dict(checkpoint['mlm_head'])\n",
    "nsp_head.load_state_dict(checkpoint['nsp_head'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_mlm_loss, train_nsp_loss = train(mlm_head, nsp_head, train_dataloader, optimizer, criterion, device)\n",
    "    valid_mlm_loss, valid_nsp_loss = evaluate(mlm_head, nsp_head, valid_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    valid_loss = valid_mlm_loss + valid_nsp_loss\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:04}')\n",
    "    print(f'Train MLM Loss: {train_mlm_loss:.4f} | Train NSP Loss: {train_nsp_loss:.4f}')\n",
    "    print(f'Valid MLM Loss: {valid_mlm_loss:.4f} | Valid NSP Loss: {valid_nsp_loss:.4f}')\n",
    "\n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(\n",
    "                {'mlm_head' : mlm_head.state_dict(),'nsp_head' : nsp_head.state_dict(),'optimizer': optimizer.state_dict()}, \n",
    "                'weights/BERT_LM_best.pt'\n",
    "            )\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        checkpoint = torch.load('weights/BERT_LM_best.pt')\n",
    "        mlm_head.load_state_dict(checkpoint['mlm_head'])\n",
    "        nsp_head.load_state_dict(checkpoint['nsp_head'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be7b3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm output : torch.Size([128, 256, 32000])\n",
      "masking label : torch.Size([128, 256])\n",
      "masking label : tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "mlm output after masking : torch.Size([128, 256, 32000])\n",
      "mlm target : torch.Size([3036])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y_mlm, segment_emb, y_nsp, masking_label) in enumerate(train_dataloader):\n",
    "    mlm_output = mlm_head(X.to(device), segment_emb.long().to(device))\n",
    "    print(f\"mlm output : {mlm_output.shape}\")\n",
    "    \n",
    "    output_dim = mlm_output.shape[-1]\n",
    "\n",
    "    #         mlm_output = mlm_output.reshape(-1, mlm_output.shape[-1])\n",
    "    #         mlm_loss   = criterion(mlm_output, y_mlm.to(device).reshape(-1)) # CE\n",
    "\n",
    "    print(f\"masking label : {masking_label.shape}\")\n",
    "    print(f\"masking label : {masking_label}\")\n",
    "\n",
    "    mlm_output_ = mlm_output[masking_label.bool().to(device)].reshape(-1, output_dim)\n",
    "    print(f\"mlm output after masking : {mlm_output.shape}\")\n",
    "    mlm_target = y_mlm.reshape(-1)[masking_label.reshape(-1).bool()].to(device)\n",
    "    print(f\"mlm target : {mlm_target.shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3faaaa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8982,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "         ...,\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]],\n",
       "\n",
       "        [[ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         ...,\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]],\n",
       "\n",
       "        [[ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "         [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "         ...,\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8981,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "         ...,\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]],\n",
       "\n",
       "        [[ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         ...,\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]],\n",
       "\n",
       "        [[ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8982,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         ...,\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8983,  0.6251,  ...,  0.5706,  0.5228,  0.5220],\n",
       "         [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "045c50ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "        [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "        [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "        ...,\n",
       "        [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "        [-0.6296,  4.9435, -0.6296,  ..., -0.5748, -0.5267, -0.5260],\n",
       "        [ 0.6251, -4.8984,  0.6251,  ...,  0.5706,  0.5228,  0.5220]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81e9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_output = torch.randn(10, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c41f05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8854e+00, -4.5087e-01, -3.0679e+00,  ...,  9.5486e-01,\n",
       "           6.8343e-01,  4.0800e-01],\n",
       "         [-2.4270e-01, -2.0843e-01, -3.7478e-01,  ...,  1.2955e+00,\n",
       "          -2.8333e-01,  3.0489e-02],\n",
       "         [ 7.1423e-02, -1.0986e+00,  1.1390e+00,  ..., -5.7042e-01,\n",
       "          -8.8368e-01,  6.9217e-01],\n",
       "         ...,\n",
       "         [ 3.2071e-01, -2.2002e+00,  8.0698e-02,  ...,  3.2466e-01,\n",
       "          -9.9349e-01,  5.0202e-01],\n",
       "         [-1.4008e+00,  5.0895e-01, -9.3579e-02,  ..., -1.1260e+00,\n",
       "          -4.9518e-01,  9.7975e-01],\n",
       "         [-3.9151e-01,  6.5946e-01,  9.1997e-01,  ..., -1.8284e+00,\n",
       "           1.1318e+00,  7.0110e-02]],\n",
       "\n",
       "        [[ 5.1004e-01,  6.4652e-01,  4.8502e-01,  ...,  8.0034e-01,\n",
       "          -5.4346e-01, -1.6462e+00],\n",
       "         [ 6.9855e-02, -9.8620e-02, -6.2837e-01,  ...,  7.2709e-01,\n",
       "          -3.3640e-01, -1.7753e+00],\n",
       "         [ 1.3028e+00, -1.6390e+00,  9.5311e-02,  ..., -5.4788e-01,\n",
       "           3.3195e-02, -1.3426e+00],\n",
       "         ...,\n",
       "         [-5.6727e-01,  8.1239e-01, -5.2894e-01,  ..., -9.9311e-03,\n",
       "          -1.6011e+00,  1.6484e+00],\n",
       "         [-4.0147e-01, -3.0158e-01,  1.3008e+00,  ..., -1.0779e+00,\n",
       "           5.4896e-01, -1.7422e+00],\n",
       "         [-1.6844e-01,  1.4874e-02,  1.3399e-01,  ...,  1.1606e+00,\n",
       "          -6.7196e-01,  1.0479e+00]],\n",
       "\n",
       "        [[-4.9973e-01,  2.3896e+00, -1.4447e+00,  ...,  8.5605e-01,\n",
       "          -5.0835e-01,  4.9490e-01],\n",
       "         [-1.1850e+00,  5.7250e-01,  1.4244e+00,  ..., -3.8088e-02,\n",
       "           1.7972e+00,  2.6162e-01],\n",
       "         [ 3.6373e-01,  1.0636e+00, -4.8507e-01,  ...,  5.7364e-01,\n",
       "          -1.9750e-01,  1.4556e+00],\n",
       "         ...,\n",
       "         [-2.6987e-01, -4.0368e-01, -1.7560e+00,  ...,  2.4699e+00,\n",
       "           1.3900e-01, -6.6945e-01],\n",
       "         [ 1.1789e+00,  9.4162e-01,  1.4554e+00,  ...,  3.0176e-01,\n",
       "           1.4543e+00, -1.1494e+00],\n",
       "         [-9.7540e-01, -1.1698e+00,  8.9600e-01,  ..., -5.9966e-01,\n",
       "           5.7506e-01, -8.0825e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.0708e-01, -4.4683e-02,  2.1188e-01,  ..., -1.0248e+00,\n",
       "           1.1476e+00, -1.6968e+00],\n",
       "         [-2.1046e-01, -6.1047e-01, -1.6243e+00,  ...,  2.2383e+00,\n",
       "           6.6884e-01, -8.1615e-02],\n",
       "         [ 3.4018e-01, -4.4457e-01, -8.3555e-01,  ...,  1.1625e-01,\n",
       "           3.8888e-01, -1.7906e+00],\n",
       "         ...,\n",
       "         [-4.2894e-01,  1.9813e+00,  9.2105e-01,  ..., -9.8422e-01,\n",
       "          -6.1015e-01, -2.8639e-01],\n",
       "         [ 8.5746e-01,  6.2076e-01,  1.1703e+00,  ..., -3.3702e-02,\n",
       "           9.3187e-01,  1.4487e+00],\n",
       "         [ 1.1019e+00, -4.8802e-01, -7.6296e-01,  ..., -2.3110e+00,\n",
       "          -6.0738e-01,  8.4426e-01]],\n",
       "\n",
       "        [[-1.3729e+00, -1.3393e+00, -1.5263e+00,  ..., -5.1832e-01,\n",
       "           1.3887e+00, -1.1607e+00],\n",
       "         [-8.1432e-01, -1.6580e-01,  1.1167e+00,  ...,  2.0953e-01,\n",
       "          -4.7866e-01,  6.5877e-01],\n",
       "         [-1.1650e+00, -3.9313e-01, -1.6003e-01,  ...,  4.7750e-01,\n",
       "           6.2438e-01,  7.9433e-01],\n",
       "         ...,\n",
       "         [-5.0064e-01, -1.5517e+00,  1.3814e+00,  ..., -2.3469e-01,\n",
       "           5.1423e-02,  8.0645e-01],\n",
       "         [ 4.9268e-01,  4.1573e-01,  2.0305e+00,  ...,  2.3747e+00,\n",
       "          -1.7980e+00, -8.5961e-01],\n",
       "         [-1.2025e+00,  2.2428e-04, -7.5084e-02,  ...,  1.2991e-01,\n",
       "          -8.9454e-01,  5.7154e-01]],\n",
       "\n",
       "        [[-1.8979e+00,  8.6094e-01,  4.2803e-01,  ..., -4.7323e-01,\n",
       "           7.6267e-01,  4.7437e-01],\n",
       "         [ 1.8319e+00, -1.1261e+00, -2.5290e-01,  ..., -7.4224e-01,\n",
       "           6.0219e-01, -2.6035e-01],\n",
       "         [-1.5023e+00,  1.6850e+00, -1.3830e+00,  ..., -9.1528e-01,\n",
       "           1.1902e+00, -2.1502e+00],\n",
       "         ...,\n",
       "         [ 1.4213e-01,  4.3117e-01, -8.4598e-02,  ..., -1.4997e+00,\n",
       "           3.3993e-01,  2.0729e-01],\n",
       "         [ 3.8899e-01,  3.0111e-01,  5.4855e-01,  ..., -1.3314e+00,\n",
       "           1.2830e+00,  2.1302e-01],\n",
       "         [-7.7891e-01, -1.6818e+00,  2.0160e+00,  ..., -1.0296e+00,\n",
       "           3.3717e-01,  2.7718e-01]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9ed1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_label        = torch.zeros(10, 256)\n",
    "masking_label[0, 1]  = 1\n",
    "masking_label[1, 12] = 1\n",
    "masking_label[2, 13] = 1\n",
    "masking_label[3, 14] = 1\n",
    "masking_label[3, 15] = 1\n",
    "mlm_output_          = mlm_output[masking_label.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1f7091d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aee3ea2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_output_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b660fb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4270e-01, -2.0843e-01, -3.7478e-01, -1.1358e+00, -2.1946e-01,\n",
       "          3.3446e-01,  5.7034e-01, -3.8152e-01, -4.6428e-01, -1.5560e+00,\n",
       "         -1.1907e+00, -1.5742e+00, -1.3044e+00, -2.2895e-01,  8.2193e-01,\n",
       "         -1.4863e-01, -2.2180e-01, -4.9838e-01, -1.3370e+00,  2.8029e-01,\n",
       "          2.1031e-01,  3.6513e-03,  3.7757e-01, -9.4559e-01, -4.5971e-01,\n",
       "          1.5870e+00,  2.1090e+00, -6.2280e-01, -3.3466e-01,  2.6692e-01,\n",
       "         -6.6542e-01, -5.7952e-01, -1.2446e+00, -9.5962e-01,  2.9304e-01,\n",
       "          3.9016e-01, -1.9302e+00, -1.7927e-01,  4.0165e-01,  4.2341e-01,\n",
       "          1.6429e-01, -7.9567e-02, -7.1369e-01,  1.0389e+00,  2.9375e-01,\n",
       "          5.9075e-01,  2.8927e-01,  8.9392e-01,  5.3192e-01, -3.5230e-01,\n",
       "          3.0128e-01,  1.8223e+00, -6.6174e-01, -2.4908e-01, -1.7012e+00,\n",
       "         -3.8633e-01,  5.7321e-01, -1.4606e+00,  1.3026e+00,  7.7289e-02,\n",
       "          4.5825e-01, -3.4294e-01, -1.7604e-01,  9.6182e-01, -7.6659e-01,\n",
       "         -1.0352e-01,  1.6431e+00, -2.1946e+00,  1.1918e-01,  1.6223e+00,\n",
       "          3.8694e-01, -6.8493e-01, -9.8069e-01,  5.0997e-02,  8.6571e-01,\n",
       "          1.3097e+00,  2.9907e-01,  4.3905e-01,  1.6131e+00, -1.2291e+00,\n",
       "          1.0452e-01, -9.1181e-02, -5.4267e-01,  1.2873e-01,  1.7574e-01,\n",
       "          8.1414e-01,  1.8361e+00,  3.5554e-03,  5.6177e-01, -1.4119e-01,\n",
       "         -2.2496e-02,  6.1988e-01,  1.1863e+00,  4.3939e-01, -1.6247e+00,\n",
       "          7.8636e-01, -3.8735e-02, -1.8398e+00, -1.0603e+00, -1.1261e+00,\n",
       "          1.0914e+00, -1.9192e+00, -1.1998e+00, -1.1897e-02, -7.1778e-01,\n",
       "          2.0201e-01, -1.4940e+00, -6.5317e-01,  2.4503e+00,  5.7740e-01,\n",
       "         -8.3405e-01, -9.1225e-01, -1.4107e-01,  8.1365e-01, -6.1904e-01,\n",
       "          4.1570e-01,  6.8565e-01,  4.3843e-01,  1.5479e+00, -2.3936e-02,\n",
       "          1.4042e+00,  1.2672e+00,  1.6224e-02,  1.5219e-01,  7.1968e-01,\n",
       "          1.2955e+00, -2.8333e-01,  3.0489e-02],\n",
       "        [-2.1512e+00,  3.0402e-01,  3.5265e-01, -1.4856e+00, -5.3873e-01,\n",
       "          1.6556e-01,  2.0903e+00,  2.1495e-01,  6.4342e-01,  2.3119e-01,\n",
       "         -6.0979e-01,  1.2010e+00,  3.4864e-01, -1.1142e+00,  2.4514e+00,\n",
       "          1.4476e+00,  1.4066e+00,  5.1381e-01,  4.5200e-01, -6.2108e-01,\n",
       "         -3.6758e-01,  6.9737e-01, -3.1414e-01,  1.1630e+00,  7.3066e-01,\n",
       "          1.4551e+00, -1.1564e-01, -1.1557e+00,  6.7938e-01,  4.6820e-01,\n",
       "          8.7537e-01,  8.5871e-01,  6.7187e-01, -1.2923e+00, -4.3459e-02,\n",
       "         -1.1197e-02,  1.7274e+00,  2.5521e-01, -8.7433e-01, -2.3393e-01,\n",
       "         -3.7292e-01,  1.2326e-01,  1.7388e+00,  6.4317e-01, -7.1149e-01,\n",
       "         -2.7354e-01, -1.2550e+00,  4.1289e-01,  8.7699e-01, -2.9642e-01,\n",
       "          1.5436e+00,  3.2025e-01,  1.5522e-01,  9.7300e-02, -9.7420e-01,\n",
       "         -7.5838e-01,  5.8278e-01, -1.6378e-01,  8.6157e-01, -8.5250e-01,\n",
       "          2.3594e-01, -6.3128e-01, -5.6364e-01, -1.7926e+00, -3.3668e-01,\n",
       "          6.5731e-01, -9.1102e-01, -1.0321e+00,  5.1936e-01,  6.3925e-01,\n",
       "         -5.0006e-01, -3.0024e-01, -1.0865e+00, -5.6162e-01, -1.0397e-01,\n",
       "          1.5577e+00,  6.5452e-01,  3.4956e-03, -1.4775e+00, -9.7722e-01,\n",
       "          1.8048e+00,  6.0663e-01, -8.5944e-01,  1.3245e+00, -7.0866e-02,\n",
       "          3.1501e+00,  1.3909e+00,  4.6144e-01,  1.3596e+00,  1.2535e+00,\n",
       "          1.8661e-01, -1.3645e+00, -4.1233e-01, -1.1215e+00, -3.5809e-01,\n",
       "          4.7112e-01,  2.3373e+00,  2.7039e-01, -7.5914e-02,  2.8584e-01,\n",
       "          6.4084e-01,  8.6788e-01, -3.9197e-01,  3.4312e-01, -1.9886e+00,\n",
       "          1.5319e-02, -8.9332e-02, -1.1603e+00,  1.6590e+00, -1.2943e+00,\n",
       "          4.9337e-01,  5.7129e-01, -8.0490e-01,  1.7096e+00,  1.6236e+00,\n",
       "          8.1488e-01, -7.5876e-01, -7.5323e-01, -1.4619e+00,  3.9887e-01,\n",
       "          1.8680e-01, -1.7622e+00,  2.2281e-01,  7.7091e-01,  1.5317e+00,\n",
       "          1.5322e+00, -2.6700e-01,  9.3059e-01],\n",
       "        [-3.0842e-01, -7.8898e-01,  1.0832e+00, -4.3231e-02,  1.1509e+00,\n",
       "          3.4940e-01, -7.7990e-02,  1.0224e+00, -1.7606e+00,  7.3066e-02,\n",
       "         -1.7656e+00,  2.8954e-01,  6.2373e-01, -3.2310e-01, -4.2667e-01,\n",
       "         -1.6316e+00, -6.9240e-01,  3.1294e-02, -3.1503e-02, -4.2552e-01,\n",
       "          9.6418e-02, -5.7413e-01, -4.2707e-01,  2.4321e-01,  3.0997e-01,\n",
       "         -1.1273e+00,  1.2745e+00,  1.6675e+00, -1.2249e-01,  7.2478e-01,\n",
       "          1.5774e+00,  1.2147e+00, -2.7179e-01,  3.3506e-01,  5.0389e-01,\n",
       "          9.7127e-01, -4.5463e-01,  9.6300e-01,  3.3646e-02, -1.0297e+00,\n",
       "          1.7165e+00,  9.5193e-01, -3.0269e+00,  1.0023e+00,  9.2836e-01,\n",
       "          8.3025e-02,  5.0848e-01, -2.1272e+00,  8.6315e-01,  6.7366e-01,\n",
       "          2.8818e-01,  2.3618e-01,  2.3090e-01,  4.2186e-02, -4.0003e-01,\n",
       "          1.3078e+00,  5.9198e-01,  2.4433e-01, -9.8401e-01, -1.8194e+00,\n",
       "          5.3135e-01,  1.5467e+00,  1.0770e-02, -7.5987e-01,  2.8269e-01,\n",
       "          6.5310e-01, -1.9078e-02,  1.8567e-01,  2.1961e+00, -1.0551e+00,\n",
       "          2.5987e-02, -4.0980e-01,  1.6752e+00, -2.3151e-02,  3.3500e-01,\n",
       "          7.4995e-01,  1.0930e+00,  2.9635e-01, -2.4366e-01,  8.0120e-01,\n",
       "          9.0543e-01, -2.2419e+00, -8.3770e-01,  7.5208e-01,  5.6831e-01,\n",
       "         -6.3983e-02, -3.2728e-01,  2.4209e-01,  1.8717e+00, -1.6697e+00,\n",
       "          1.0512e+00, -3.8288e-01,  1.3702e+00, -9.4088e-01, -1.3958e-01,\n",
       "          8.1607e-01,  1.7594e+00,  4.3741e-01,  2.5231e+00, -1.0849e+00,\n",
       "         -1.2389e+00,  7.3600e-01, -5.4947e-01,  5.0241e-01, -2.7416e+00,\n",
       "         -9.7167e-01, -1.1251e+00,  2.7165e-01,  1.7587e-01,  4.2958e-01,\n",
       "          1.7089e-01,  9.8081e-02, -3.7452e-02, -1.2263e+00, -5.0827e-01,\n",
       "         -9.8872e-01,  6.3932e-01, -8.9865e-01,  1.2637e+00, -7.7021e-01,\n",
       "          1.7772e+00,  1.0053e+00,  5.4064e-01, -1.2797e+00,  6.7538e-01,\n",
       "         -2.1774e+00,  3.5069e-01,  3.3512e-01],\n",
       "        [-2.1869e+00, -1.5115e-01,  6.1741e-01,  6.4204e-01,  6.2959e-01,\n",
       "          1.4081e+00,  2.8725e-01,  4.7086e-02, -2.2752e-01, -2.0648e+00,\n",
       "         -5.4131e-02,  2.6696e+00, -5.5276e-01,  3.9623e-01, -6.8835e-01,\n",
       "          2.2739e-01,  2.3906e-01, -8.6336e-01,  2.6647e-01, -8.3376e-01,\n",
       "          2.5005e-01, -1.5002e+00, -7.9223e-01,  9.1439e-01,  6.2818e-01,\n",
       "         -6.8802e-02,  1.7508e-01,  1.9438e+00,  1.5569e+00, -6.9262e-01,\n",
       "          9.3020e-01, -2.2210e+00,  1.2001e+00,  8.5175e-01, -7.1801e-01,\n",
       "         -7.2161e-01, -5.0900e-01,  7.9514e-01, -1.4529e+00, -1.1129e+00,\n",
       "         -3.6447e-02, -2.0366e+00,  1.1766e+00,  4.1610e-01, -1.6507e+00,\n",
       "         -7.7735e-01,  7.1868e-01,  5.5839e-02, -1.8270e-01, -6.5723e-01,\n",
       "         -5.9081e-01,  3.5192e-01,  1.2487e+00,  2.7311e-01, -1.4203e+00,\n",
       "         -1.3650e+00,  2.9039e-01,  2.0745e-01,  5.8330e-01, -7.4015e-01,\n",
       "          1.2061e+00, -1.9140e-01, -2.0226e+00,  3.6260e-01, -7.0837e-01,\n",
       "          1.3745e+00, -1.4376e+00, -8.8643e-01,  3.3655e-02,  5.4409e-01,\n",
       "          7.0622e-02,  2.9225e-01, -5.0569e-01,  1.1688e+00, -7.1987e-01,\n",
       "         -6.2489e-01, -1.9531e+00,  7.0496e-01,  4.9283e-01, -1.5911e-01,\n",
       "         -2.3218e-01,  5.5081e-01,  1.6755e-01, -9.6188e-01,  1.6335e-01,\n",
       "          8.0856e-01, -3.7843e-01,  2.1999e-01,  1.6906e+00,  4.3425e-01,\n",
       "          1.4771e+00,  3.2362e-03,  3.7785e-01,  2.0211e+00,  8.7573e-01,\n",
       "         -2.2844e+00, -3.2578e-01,  4.9918e-01, -9.7839e-01, -6.1654e-01,\n",
       "          1.5074e+00,  1.0177e+00, -9.6160e-01,  6.7540e-02,  1.6343e-02,\n",
       "          1.0671e-01,  2.0143e+00, -7.5698e-01, -6.6252e-01, -6.8885e-01,\n",
       "         -7.2101e-01,  5.5166e-01,  5.7425e-01, -6.5820e-01, -2.7330e-01,\n",
       "         -7.1150e-01,  1.9031e+00, -7.3536e-01,  1.5022e+00, -4.0142e-01,\n",
       "         -1.8011e+00,  6.2649e-01,  3.6968e-01, -6.3824e-01,  1.2176e-01,\n",
       "          2.0798e-01, -2.8760e-01, -9.4598e-02],\n",
       "        [-9.6848e-02, -8.5930e-01, -1.6287e+00,  1.4244e-01, -1.5493e+00,\n",
       "         -1.5252e+00, -1.0176e+00,  8.7982e-01, -5.7959e-01,  7.7253e-01,\n",
       "          7.2668e-01, -3.4615e-01,  2.7532e-01,  5.7218e-01, -4.8389e-01,\n",
       "          4.1066e-01, -9.0516e-01,  9.7278e-02, -7.4546e-01,  8.8865e-01,\n",
       "         -9.1762e-01, -1.3057e-01,  9.4843e-02, -1.1683e+00,  4.3195e-02,\n",
       "          1.2180e+00, -4.9213e-02, -4.4732e-01,  8.3438e-01, -1.9096e-04,\n",
       "          1.7853e+00,  5.0002e-01, -1.3523e+00,  6.7832e-01, -8.9940e-01,\n",
       "          1.2671e+00,  1.1545e+00, -2.2207e+00, -1.4871e-02,  3.6339e-01,\n",
       "         -4.9468e-01,  3.6604e-01,  9.3556e-01,  5.4148e-02,  7.2162e-01,\n",
       "          9.6242e-01, -2.0733e-01,  2.6264e-01,  1.9186e+00,  4.7966e-01,\n",
       "         -9.1861e-01,  5.4040e-01, -1.9757e-01, -1.4657e-01,  1.1633e-01,\n",
       "         -4.5149e-01, -2.8363e-01,  1.8596e-02,  1.3326e-01,  4.3007e-01,\n",
       "          1.3430e-01, -6.6127e-01,  3.5125e-01, -1.5197e+00,  2.1445e-01,\n",
       "         -9.5357e-02, -2.4027e+00,  1.8113e-01,  7.9890e-01,  9.6392e-01,\n",
       "          1.1834e+00, -5.5629e-01,  6.2232e-02,  1.1125e+00,  8.8033e-01,\n",
       "          7.9178e-01,  1.3675e+00, -2.0471e+00, -1.1073e+00, -8.1484e-01,\n",
       "          5.4675e-01, -7.2860e-01, -1.6676e-01,  5.2243e-01, -1.0369e-01,\n",
       "         -9.0398e-01,  1.1788e+00,  1.1739e+00, -4.1368e-01, -5.8536e-01,\n",
       "         -9.5124e-01, -4.3064e-01, -1.4508e+00,  1.2628e+00,  6.4140e-01,\n",
       "          7.5995e-01,  1.3184e+00,  1.9775e+00, -1.8188e-01,  1.0674e+00,\n",
       "         -1.7746e-01, -1.7265e-01, -7.8356e-01, -6.6580e-01, -1.9690e-01,\n",
       "          1.4461e+00,  1.8353e+00,  6.6047e-01,  2.9785e-01, -1.2188e+00,\n",
       "          1.7718e+00, -9.2516e-01,  4.2903e-01, -1.1092e+00,  5.5493e-01,\n",
       "          1.0454e-01,  1.4299e+00,  4.6880e-03,  1.5869e-01,  1.6223e-02,\n",
       "          3.6877e-01, -1.0514e+00,  1.4965e+00, -6.7596e-01,  6.1747e-01,\n",
       "         -2.8902e-01, -4.6507e-01,  1.7583e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1859ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
