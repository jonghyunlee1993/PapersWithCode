{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a181585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "input_file = \"data/petitions_splited_mecab.txt\"\n",
    "vocab_size = 23000\n",
    "\n",
    "sp_model_root  = \"data/sentencepiece\"\n",
    "\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "\n",
    "sp_model_name        = 'tokenizer_%d'\n",
    "sp_model_path        = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type           = 'unigram'\n",
    "character_coverage   = 1.0\n",
    "user_defined_symbols = '[BOS],[EOS]'\n",
    "\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, sp_model_path, vocab_size, user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8726682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 391, 792, 1541, 4357, 886, 236, 1697, 326, 929]\n",
      "['▁나', '는', '▁오늘', '▁아침', '밥', '을', '▁먹', '었', '다', '.']\n",
      "나는 오늘 아침밥을 먹었다.\n",
      "나는 오늘 아침밥을 먹었다.\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(sp_model_path))\n",
    "\n",
    "tokens = sp.encode_as_pieces('나는 오늘 아침밥을 먹었다.')\n",
    "ids = sp.encode_as_ids('나는 오늘 아침밥을 먹었다.')\n",
    "\n",
    "print(ids)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sp.decode_pieces(tokens)\n",
    "ids = sp.decode_ids(ids)\n",
    "\n",
    "print(ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9980eda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국민과 소통하시고 자유롭고 행복한 나라를 만들기 위해 힘쓰고 계신 대통령께 존경과 찬사를 올립니다',\n",
       " '기해년 새해 복 많이 받으십시오',\n",
       " '저는 경북 울진군 북면 부구검성로 12번지에 살고 있는 북면발전협의회장 이희국이라고 합니다',\n",
       " '저는 8기의 원전이 가동․건설되고 있는 이곳 북면에 태어나 68년째 거주하고 있는 원전지역 주민입니다',\n",
       " '간절한 마음을 담아 대통령께 다음과 같이 호소 드립니다',\n",
       " '‘울진군민과 약속한 신한울 3,4호기 원전건설을 재개해 주십시오.’ 여태껏 단 한 번도 원전 건설을 원한 적 없는 제가 신한울 3,4호기 원전 건설을 청하는 까닭을 말씀드리겠습니다',\n",
       " '경상북도 동해안 최북단 울진군은 예부터 산과 바다, 계곡의 울창함이 보배처럼 아름답다하여 “울진(蔚珍)”이라는 지명을 간직하게 된 곳입니다',\n",
       " '이러한 곳에 1981년 원전사업의 시작으로 울진군에 북면(6기), 산포지구(6기), 직산지구(6기)가 원전 예정지역으로 지정되면서, 먼저 북면 부구리 지역에 원전 6기가 건설되었습니다',\n",
       " '해안선이 잘려나가고 마을 한복판에 고압 송전탑이 들어섰습니다',\n",
       " '어장이 파괴되고 지역 특산품에 방사능 꼬리표가 붙었습니다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "\n",
    "# mecab = Mecab().morphs\n",
    "mecab = Mecab()\n",
    "\n",
    "with open(\"data/petitions_2019-01\", 'r') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "sublist   = [line.split('\"content\": \"')[-1].split('\", \"num_agree\": ')[0] for line in data]\n",
    "sublist   = [doc.split(\". \") for doc in sublist]\n",
    "petitions = list(itertools.chain.from_iterable(sublist))\n",
    "petitions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0a250fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155357/155357 [00:02<00:00, 66503.90it/s]\n"
     ]
    }
   ],
   "source": [
    "total_morphs = []\n",
    "\n",
    "for sentence in tqdm(petitions):\n",
    "    morph_sentence = []\n",
    "    \n",
    "    for word in sentence.split(\" \"):\n",
    "        count = 0\n",
    "        for token, pos in mecab.pos(word):\n",
    "            if count > 0:\n",
    "                if not pos.startswith(\"N\"):\n",
    "                    token = \"##\" + token\n",
    "                    morph_sentence.append(token)\n",
    "                else:\n",
    "                    morph_sentence.append(token)\n",
    "            else:\n",
    "                morph_sentence.append(token)\n",
    "                count += 1\n",
    "        break\n",
    "    total_morphs.append(morph_sentence)\n",
    "    \n",
    "total_morphs = list(itertools.chain.from_iterable(total_morphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc681728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국민',\n",
       " '##과',\n",
       " '기',\n",
       " '해년',\n",
       " '저',\n",
       " '##는',\n",
       " '저',\n",
       " '##는',\n",
       " '간절',\n",
       " '##한',\n",
       " '‘',\n",
       " '울진',\n",
       " '군민',\n",
       " '##과',\n",
       " '경상북도',\n",
       " '이러',\n",
       " '##한',\n",
       " '해안선',\n",
       " '##이',\n",
       " '어',\n",
       " '장이',\n",
       " '삶',\n",
       " '##의',\n",
       " '그러',\n",
       " '##던',\n",
       " '저',\n",
       " '##를',\n",
       " '울진',\n",
       " '군민',\n",
       " '##과',\n",
       " '그것',\n",
       " '##이',\n",
       " '울진군',\n",
       " '민은',\n",
       " '2010',\n",
       " '년',\n",
       " '하',\n",
       " '##지만',\n",
       " '울진',\n",
       " '군민',\n",
       " '##에게',\n",
       " '##는',\n",
       " '단',\n",
       " '그',\n",
       " '지난',\n",
       " '정부',\n",
       " '의',\n",
       " '경기',\n",
       " '##가',\n",
       " '건설',\n",
       " '기존',\n",
       " '종잣돈',\n",
       " '##을',\n",
       " '건설',\n",
       " '장비',\n",
       " '##를',\n",
       " '신',\n",
       " '한울',\n",
       " '존경',\n",
       " '##하',\n",
       " '##는',\n",
       " '저',\n",
       " '##는',\n",
       " '정치',\n",
       " '##적',\n",
       " '단지',\n",
       " '친애',\n",
       " '##하',\n",
       " '##는',\n",
       " '부디',\n",
       " '지역',\n",
       " '주',\n",
       " '민의',\n",
       " '지난',\n",
       " '대통령',\n",
       " '##은',\n",
       " '약속',\n",
       " '##을',\n",
       " '간곡히',\n",
       " '2019',\n",
       " '년',\n",
       " '북면',\n",
       " '발전',\n",
       " '협의',\n",
       " '회장',\n",
       " '2008',\n",
       " '년',\n",
       " '대한민국',\n",
       " '11',\n",
       " '년',\n",
       " '지난',\n",
       " '농업',\n",
       " '##경',\n",
       " '영체',\n",
       " '돌아온',\n",
       " '신고',\n",
       " '새',\n",
       " '아직',\n",
       " '대한민국',\n",
       " '존경']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_morphs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b68b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/petition_proc_with_hash_code.txt\", 'w') as f:\n",
    "    f.writelines(total_morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d125e8d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertWordPieceTokenizer\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer, SentencePieceBPETokenizer, CharBPETokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "how_to_tokenize = BertWordPieceTokenizer\n",
    "\n",
    "if str(how_to_tokenize) == str(BertWordPieceTokenizer):\n",
    "    print('BertWordPieceTokenizer')\n",
    "    tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False)\n",
    "elif str(how_to_tokenize) == str(SentencePieceBPETokenizer):\n",
    "    print('SentencePieceBPETokenizer')\n",
    "    tokenizer = SentencePieceBPETokenizer()\n",
    "elif str(how_to_tokenize) == str(CharBPETokenizer):\n",
    "    print('CharBPETokenizer')\n",
    "    tokenizer = CharBPETokenizer() \n",
    "elif str(how_to_tokenize) == str(ByteLevelBPETokenizer):\n",
    "    print('ByteLevelBPETokenizer')\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "else:\n",
    "    assert('select right tokenizer')\n",
    "\n",
    "corpus_file    = ['data/petition_proc_with_hash_code.txt']  # data path\n",
    "vocab_size     = 32000\n",
    "limit_alphabet = 6000\n",
    "output_path    = 'hugging_%d'%(vocab_size)\n",
    "min_frequency  = 5\n",
    "\n",
    "# Then train it!\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet,\n",
    "               show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "116706e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 오늘 아침밥을 먹었다.\n",
      "=>idx   : [447, 1946, 4318, 1045, 1958, 1928, 1957, 749, 2612, 1672, 17]\n",
      "=>tokens: ['나', '##는', '오늘', '아', '##침', '##밥', '##을', '먹', '##었', '##다', '.']\n",
      "=>offset: [(0, 1), (1, 2), (3, 5), (6, 7), (7, 8), (8, 9), (9, 10), (11, 12), (12, 13), (13, 14), (14, 15)]\n",
      "=>decode: 나는 오늘 아침밥을 먹었다.\n",
      "\n",
      "I want to go my hometown\n",
      "=>idx   : [44, 90, 4079, 2045, 87, 1891, 4268, 80, 2306, 75, 1891, 2238, 5012, 7979, 2152]\n",
      "=>tokens: ['I', 'w', '##an', '##t', 't', '##o', 'go', 'm', '##y', 'h', '##o', '##m', '##et', '##ow', '##n']\n",
      "=>offset: [(0, 1), (2, 3), (3, 5), (5, 6), (7, 8), (8, 9), (10, 12), (13, 14), (14, 15), (16, 17), (17, 18), (18, 19), (19, 21), (21, 23), (23, 24)]\n",
      "=>decode: I want to go my hometown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '나는 오늘 아침밥을 먹었다.'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(sentence)\n",
    "print('=>idx   : %s'%output.ids)\n",
    "print('=>tokens: %s'%output.tokens)\n",
    "print('=>offset: %s'%output.offsets)\n",
    "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
    "\n",
    "sentence = 'I want to go my hometown'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(sentence)\n",
    "print('=>idx   : %s'%output.ids)\n",
    "print('=>tokens: %s'%output.tokens)\n",
    "print('=>offset: %s'%output.offsets)\n",
    "print('=>decode: %s\\n'%tokenizer.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8f760e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tokenizer_model/vocab.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_path='data/tokenizer_model'\n",
    "\n",
    "if not os.path.isdir(hf_model_path):\n",
    "    os.mkdir(hf_model_path)\n",
    "\n",
    "tokenizer.save_model(hf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17f91414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 12432\n",
      "Tokens (str)      : ['[CLS]', '나', '##는', '오늘', '아', '##침', '##밥', '##을', '먹', '##었', '##다', '.', '[SEP]']\n",
      "Tokens (int)      : [2, 447, 1946, 4318, 1045, 1958, 1928, 1957, 749, 2612, 1672, 17, 3]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer_for_load = BertTokenizerFast.from_pretrained(hf_model_path,\n",
    "                                                       strip_accents=False,\n",
    "                                                       lowercase=False)\n",
    "\n",
    "print('vocab size : %d' % tokenizer_for_load.vocab_size)\n",
    "tokenized_input_for_pytorch = tokenizer_for_load(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
    "\n",
    "print(\"Tokens (str)      : {}\".format([tokenizer_for_load.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
    "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
    "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "286a94e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_for_load.all_special_tokens # 추가하기 전 기본적인 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7423f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
