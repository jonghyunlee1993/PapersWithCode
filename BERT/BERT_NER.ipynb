{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5817c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fpath = './data/tokenizer_model'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(fpath,\n",
    "                                              strip_accents=False,\n",
    "                                              lowercase=False)\n",
    "\n",
    "vocab_dim     = len(tokenizer.vocab)\n",
    "seq_len       = 256\n",
    "embedding_dim = 512\n",
    "device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size    = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b11e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/GyuminJack/torchstudy/blob/main/06Jun/NER/src/data.py\n",
    "\n",
    "import linecache\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    loaded_tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, strip_accents=False, lowercase=False)  # Must be False if cased model  # 로드\n",
    "    return loaded_tokenizer\n",
    "\n",
    "class KlueDataset_NER(Dataset):\n",
    "    def __init__(self, vocab_txt_path, txt_path, *args, **kwargs):\n",
    "        self.tokenizer = load_tokenizer(vocab_txt_path)\n",
    "        self.max_seq_len = 256\n",
    "        self.txt_path = txt_path\n",
    "        \n",
    "        self.cls_token_id  = self.tokenizer.cls_token_id\n",
    "        self.sep_token_id  = self.tokenizer.sep_token_id\n",
    "        self.pad_token_id  = self.tokenizer.pad_token_id\n",
    "        \n",
    "        self.bio_dict = {\n",
    "                        '[PAD]' : 0,\n",
    "                        'B-DT': 1,\n",
    "                        'B-LC': 2,\n",
    "                        'B-OG': 3,\n",
    "                        'B-PS': 4,\n",
    "                        'B-QT': 5,\n",
    "                        'B-TI': 6,\n",
    "                        'I-DT': 7,\n",
    "                        'I-LC': 8,\n",
    "                        'I-OG': 9,\n",
    "                        'I-PS': 10,\n",
    "                        'I-QT': 11,\n",
    "                        'I-TI': 12,\n",
    "                        'O': 13\n",
    "                        }\n",
    "        self.reverse_bio_dict = {v:k for k, v in self.bio_dict.items()}\n",
    "        with open(self.txt_path, \"r\") as f:\n",
    "            self._total_data = len(f.readlines())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_ko = linecache.getline(self.txt_path, idx + 1).strip()\n",
    "        text, bio_string = raw_ko.split(\"\\t\")\n",
    "        bio_tensor = [self.bio_dict[i] for i in bio_string.split(\",\")]\n",
    "        \n",
    "        sent = self.tokenizer.encode(text)[1:-1]\n",
    "        pad_length = self.max_seq_len - len(sent)\n",
    "        \n",
    "        train = torch.tensor([self.cls_token_id] + sent + [self.sep_token_id] + [self.pad_token_id] * pad_length).long().contiguous()\n",
    "        target = torch.tensor(bio_tensor + [self.pad_token_id] * pad_length).long().contiguous()\n",
    "        \n",
    "        segment_embedding = torch.zeros(target.size(0)).long()\n",
    "        \n",
    "        return train, target, segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8973d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inhyeokyoo.github.io/project/nlp/bert-issue/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, pad_token_id):\n",
    "        super(BERT, self).__init__()\n",
    "        self.pad_token_id  = pad_token_id\n",
    "        self.nhead         = 8\n",
    "        self.embedding     = BERTEmbedding(vocab_dim, seq_len, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=self.nhead, batch_first=True)\n",
    "        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        pad_mask  = BERT.get_attn_pad_mask(data, data, self.pad_token_id).repeat(self.nhead, 1, 1)\n",
    "        embedding = self.embedding(data, segment_embedding)\n",
    "        output    = self.encoder_block(embedding, pad_mask) \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "        batch_size, len_q = seq_q.size()\n",
    "        batch_size, len_k = seq_k.size()\n",
    "        pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "        pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "        \n",
    "        return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8056e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim, seq_len, embedding_dim, dropout_rate=0.1, device=device):\n",
    "        super(BERTEmbedding, self).__init__()\n",
    "        self.seq_len       = seq_len\n",
    "        self.vocab_dim     = vocab_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate  = dropout_rate\n",
    "        \n",
    "        # vocab --> embedding\n",
    "        self.token_embedding      = nn.Embedding(self.vocab_dim, self.embedding_dim) \n",
    "        self.token_dropout        = nn.Dropout(self.dropout_rate)    \n",
    "        \n",
    "        # seq len --> embedding\n",
    "        self.positional_embedding = nn.Embedding(self.seq_len, self.embedding_dim)\n",
    "        self.positional_dropout   = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        # segment (0, 1) --> embedding\n",
    "        self.segment_embedding    = nn.Embedding(2, self.embedding_dim)\n",
    "        self.segment_dropout      = nn.Dropout(self.dropout_rate) \n",
    "        \n",
    "        \n",
    "    def forward(self, data, segment_embedding):\n",
    "        token_embedding      = self.token_embedding(data)\n",
    "        token_embedding      = self.token_dropout(token_embedding)\n",
    "        \n",
    "        positional_encoding  = torch.arange(start=0, end=self.seq_len, step=1).long()\n",
    "        # data의 device 정보 가져와서 처리\n",
    "        positional_encoding  = positional_encoding.unsqueeze(0).expand(data.size()).to(device)\n",
    "        positional_embedding = self.positional_embedding(positional_encoding)\n",
    "        positional_embedding = self.positional_dropout(positional_embedding)\n",
    "        \n",
    "        segment_embedding    = self.segment_embedding(segment_embedding)\n",
    "        segment_embedding    = self.segment_dropout(segment_embedding)\n",
    "        \n",
    "        return token_embedding + positional_embedding + segment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7873260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class BertNER(nn.Module):\n",
    "    def __init__(self, bert, bert_hidden_size, num_classes, use_LSTM=True):\n",
    "        super().__init__()\n",
    "        self.use_LSTM = use_LSTM\n",
    "        self.bert     = bert\n",
    "        self.fc       = nn.Linear(bert_hidden_size, num_classes)\n",
    "        self.dropout  = nn.Dropout(0.1)\n",
    "        self.lstm     = nn.LSTM(bert_hidden_size, num_classes, batch_first=True)\n",
    "        self.crf      = CRF(num_tags=num_classes, batch_first=True)\n",
    "\n",
    "    def forward(self, source, target, segment_embedding):\n",
    "        source              = self.bert(source, segment_embedding)\n",
    "        last_encoder_output = self.dropout(source)\n",
    "        \n",
    "        if self.use_LSTM:\n",
    "            last_encoder_output, _ = self.lstm(last_encoder_output)\n",
    "\n",
    "        emissions              = self.fc(last_encoder_output)\n",
    "        log_likelihood, output = self.crf(emissions, target), self.crf.decode(emissions)\n",
    "        \n",
    "        return log_likelihood, torch.tensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cecabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/long8v/torch_study/blob/master/paper/06_BERT/source/finetune/ner_bert.py\n",
    "\n",
    "def compute_metrics(y_pred_, y_test_):\n",
    "    y_pred = torch.Tensor(y_pred_).view(-1).to('cpu')\n",
    "    y_test = torch.Tensor(y_test_).view(-1).to('cpu')\n",
    "    \n",
    "    y_pred = y_pred[y_test != self.pad_idx]\n",
    "    y_test = y_test[y_test != self.pad_idx]\n",
    "    \n",
    "    micro_score = f1_score(y_pred, y_test, average='micro')\n",
    "    macro_score = f1_score(y_pred, y_test, average=None)\n",
    "    accuracy_score = accuracy_score(y_pred, y_test)\n",
    "    \n",
    "    # macro에서 pad_idx에 대한 값은 평균 구할 때 빼줌\n",
    "    macro_score = np.mean([score for idx, score in enumerate(macro_score) if idx != self.pad_idx])\n",
    "\n",
    "    return {'accuracy' : accuracy_score, 'f1_micro': micro_score, 'f1_macro': macro_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e413630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, device, clip=1):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc  = 0\n",
    "    epoch_f1   = 0\n",
    "    \n",
    "    for source, target, segment_embedding in tqdm(iterator, total=len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "        segment_embedding = segment_embedding.to(device)\n",
    "\n",
    "        log_likelihood, output = model(source, target, segment_embedding) \n",
    "        \n",
    "        loss = -1 * log_likelihood\n",
    "        \n",
    "        loss.backward()\n",
    "                \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        metrics = compute_metrics(output, target)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc  += metrics['accuracy']\n",
    "        epoch_f1   += metrics['f1_macro']\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, optimizer, device, clip=1):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc  = 0\n",
    "    epoch_f1   = 0\n",
    "    \n",
    "    for source, target, segment_embedding in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "        segment_embedding = segment_embedding.to(device)\n",
    "\n",
    "        log_likelihood, output = model(source, target, segment_embedding) \n",
    "\n",
    "        loss = -1 * log_likelihood\n",
    "        \n",
    "        metrics = compute_metrics(output, target)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc  += metrics['accuracy']\n",
    "        epoch_f1   += metrics['f1_macro']\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb012fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_txt_path = \"./data/tokenizer_model\"\n",
    "\n",
    "train_path = \"./data/klue_ner_processed.train\"\n",
    "train_dataset = KlueDataset_NER(vocab_txt_path, train_path)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_path = \"./data/klue_ner_processed.dev\"\n",
    "valid_dataset = KlueDataset_NER(vocab_txt_path, valid_path)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "checkpoint = torch.load('./weights/BERT_LM_best.pt')\n",
    "bert = BERT(vocab_dim=vocab_dim, seq_len=seq_len, embedding_dim=embedding_dim, pad_token_id=0).to(device)\n",
    "bert.load_state_dict(checkpoint['bert'])\n",
    "\n",
    "ner_head = BertNER(bert, embedding_dim, len(train_dataset.bio_dict)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(ner_head.parameters(), lr = 0.0001, weight_decay = 0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b32e2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822d69c3540c407fa319a9ad89e8bd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/aten/src/ATen/native/TensorCompare.cpp:255.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Train Loss: 10316.1697 | Train Acc: 0.8016\n",
      "Valid Loss: 2992.3516 | Valid Acc: 0.8855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e54cbdbd1d0436e832a8ac3a32eb81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0002\n",
      "Train Loss: 2398.7176 | Train Acc: 0.9062\n",
      "Valid Loss: 2395.5992 | Valid Acc: 0.9075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471a4beb040046a69365a32fda659525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0003\n",
      "Train Loss: 1931.0802 | Train Acc: 0.9225\n",
      "Valid Loss: 2126.0312 | Valid Acc: 0.9158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f3df9bbc9f46dc8014ea31cba1c8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0004\n",
      "Train Loss: 1649.6522 | Train Acc: 0.9327\n",
      "Valid Loss: 1967.0039 | Valid Acc: 0.9224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc6ab6c658a4ad2852fc3dd919500ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005\n",
      "Train Loss: 1454.7549 | Train Acc: 0.9405\n",
      "Valid Loss: 1887.0495 | Valid Acc: 0.9271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67165a4bd848426b9c7b06926ffea34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0006\n",
      "Train Loss: 1302.2208 | Train Acc: 0.9464\n",
      "Valid Loss: 1821.0159 | Valid Acc: 0.9295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaaf77728fb4119998eca81eaf93c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0007\n",
      "Train Loss: 1177.8361 | Train Acc: 0.9510\n",
      "Valid Loss: 1825.4787 | Valid Acc: 0.9291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be734202a849b28a7df1a4f94d5b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008\n",
      "Train Loss: 1075.4959 | Train Acc: 0.9551\n",
      "Valid Loss: 1776.8044 | Valid Acc: 0.9325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deabe56cf5f4519815d9632f08e028b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0009\n",
      "Train Loss: 987.2065 | Train Acc: 0.9585\n",
      "Valid Loss: 1767.2415 | Valid Acc: 0.9348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f0d8044bf642c08a3aef0b52a6dbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010\n",
      "Train Loss: 913.8428 | Train Acc: 0.9613\n",
      "Valid Loss: 1757.3239 | Valid Acc: 0.9366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e24012ce28f43b29839356b9d42b589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0011\n",
      "Train Loss: 836.5510 | Train Acc: 0.9643\n",
      "Valid Loss: 1752.3896 | Valid Acc: 0.9374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6412c35bb524f9a876d73dc7a5f66db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0012\n",
      "Train Loss: 776.9342 | Train Acc: 0.9665\n",
      "Valid Loss: 1768.3807 | Valid Acc: 0.9389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6926b7a3e0461ca40d43a13db6a3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0013\n",
      "Train Loss: 724.6691 | Train Acc: 0.9689\n",
      "Valid Loss: 1764.1499 | Valid Acc: 0.9395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227de06223104fb0b98a4e4c188a46a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0014\n",
      "Train Loss: 667.8331 | Train Acc: 0.9710\n",
      "Valid Loss: 1792.0772 | Valid Acc: 0.9400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8b44a8866f4ef4b7002a7c80430a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0015\n",
      "Train Loss: 618.8147 | Train Acc: 0.9732\n",
      "Valid Loss: 1773.0927 | Valid Acc: 0.9403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd0f823c5d34e3ab50fd7e391a4d36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0016\n",
      "Train Loss: 577.9724 | Train Acc: 0.9749\n",
      "Valid Loss: 1835.9813 | Valid Acc: 0.9406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a2f8e21db74209aa461d8d1a3e8831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0017\n",
      "Train Loss: 541.7198 | Train Acc: 0.9763\n",
      "Valid Loss: 1870.0714 | Valid Acc: 0.9399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e751aa394f8048df9d7ecf5d9b26f325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0018\n",
      "Train Loss: 508.3576 | Train Acc: 0.9776\n",
      "Valid Loss: 1847.8342 | Valid Acc: 0.9413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7f686231b49bab410e290bfb55e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0019\n",
      "Train Loss: 484.3505 | Train Acc: 0.9787\n",
      "Valid Loss: 1904.7685 | Valid Acc: 0.9407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4fdcb4e5c54fb1a342a6a70e186d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020\n",
      "Train Loss: 452.2369 | Train Acc: 0.9804\n",
      "Valid Loss: 1892.4936 | Valid Acc: 0.9421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b49cc858db4c63b2e0e8cfaf6ad5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0021\n",
      "Train Loss: 423.5917 | Train Acc: 0.9813\n",
      "Valid Loss: 1962.9563 | Valid Acc: 0.9426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84a4d343f4d4c868c3671e6fd26dbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0022\n",
      "Train Loss: 410.6223 | Train Acc: 0.9819\n",
      "Valid Loss: 1902.5098 | Valid Acc: 0.9432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb94d3413b654e3e8d8f9f945c917ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0023\n",
      "Train Loss: 370.4296 | Train Acc: 0.9838\n",
      "Valid Loss: 1968.0586 | Valid Acc: 0.9429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1128517d1b8243ebb2eebb1f0a035cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0024\n",
      "Train Loss: 359.2646 | Train Acc: 0.9843\n",
      "Valid Loss: 1978.6798 | Valid Acc: 0.9434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a153914271d744a39a88804ec4c2d94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0025\n",
      "Train Loss: 352.9422 | Train Acc: 0.9845\n",
      "Valid Loss: 1988.0291 | Valid Acc: 0.9436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab9f6dc5bf946cc9b24179873e5d5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0026\n",
      "Train Loss: 344.6962 | Train Acc: 0.9848\n",
      "Valid Loss: 2008.5164 | Valid Acc: 0.9435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9508deb0a3743cfbb7330fd35379924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0027\n",
      "Train Loss: 348.4822 | Train Acc: 0.9848\n",
      "Valid Loss: 1997.6107 | Valid Acc: 0.9435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc745971fcc648f1ad5939c3772e255f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0028\n",
      "Train Loss: 345.2984 | Train Acc: 0.9846\n",
      "Valid Loss: 1995.3747 | Valid Acc: 0.9436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bf9cd373404622a6ce234187e39a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0029\n",
      "Train Loss: 336.5549 | Train Acc: 0.9850\n",
      "Valid Loss: 2010.8467 | Valid Acc: 0.9436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e44c77b76418fb0ed1c98227b6923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0030\n",
      "Train Loss: 327.5774 | Train Acc: 0.9856\n",
      "Valid Loss: 2016.5985 | Valid Acc: 0.9434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667acd923b6641d7b939d8eb854da742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0031\n",
      "Train Loss: 330.1552 | Train Acc: 0.9856\n",
      "Valid Loss: 2026.9738 | Valid Acc: 0.9434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1523342da13640519f270675b24e7ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0032\n",
      "Train Loss: 326.0173 | Train Acc: 0.9858\n",
      "Valid Loss: 2054.1785 | Valid Acc: 0.9440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a9543119dc4966a07ad9e7b4cd4141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0033\n",
      "Train Loss: 327.8226 | Train Acc: 0.9858\n",
      "Valid Loss: 2043.4496 | Valid Acc: 0.9438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f28b1fe87749e2a88841d3fcea32ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0034\n",
      "Train Loss: 322.5582 | Train Acc: 0.9861\n",
      "Valid Loss: 2038.4425 | Valid Acc: 0.9439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fa2e76d16746568500383b036b7498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0035\n",
      "Train Loss: 318.5146 | Train Acc: 0.9861\n",
      "Valid Loss: 2038.6860 | Valid Acc: 0.9439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df97f4a9d4ee4570b8cf6c30876a75e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7c1fc1d1ab1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-aba090a6f8a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, device, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msegment_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-870c35edcf11>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, segment_embedding)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlast_encoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0memissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_encoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     def _validate(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36m_viterbi_decode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# sequence, and trace it back again, and so on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_ends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mbest_last_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mbest_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_last_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "N_EPOCHS  = 1000\n",
    "PAITIENCE = 30\n",
    "\n",
    "n_paitience = 0\n",
    "best_valid_loss = float('inf')\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train(ner_head, train_data_loader, optimizer, device)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(ner_head, valid_data_loader, optimizer, device)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:04}')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1: .4f}')\n",
    "    print(f'Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f} | Valid F1: {valid_f1: .4f}')\n",
    "\n",
    "    with open(\"output/log_ner.txt\", \"a\") as f:\n",
    "        f.write(\"epoch: {0:04d} \\ \n",
    "                train loss: {1:.4f}, train acc: {2:.4f}, train_f1: {3:.4f}   \\\n",
    "                valid loss: {4:.4f}, valid acc: {5:.4f}, valid_f1: {6: .4f}} \\n\".format(epoch, train_loss, train_acc, train_f1, valid_loss, valid_acc, valid_f1))\n",
    "\n",
    "\n",
    "    if n_paitience < PAITIENCE:\n",
    "        if best_valid_loss > valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                'ner_head' : ner_head.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "                }, 'weights/BERT_ner_best.pt'\n",
    "            )\n",
    "            n_paitience = 0\n",
    "        elif best_valid_loss <= valid_loss:\n",
    "            n_paitience += 1\n",
    "    else:\n",
    "        print(\"Early stop!\")\n",
    "        checkpoint = torch.load('weights/BERT_ner_best.pt')\n",
    "        ner_head.load_state_dict(checkpoint['ner_head'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac12dd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): BERTEmbedding(\n",
       "    (token_embedding): Embedding(32000, 512)\n",
       "    (token_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (positional_embedding): Embedding(256, 512)\n",
       "    (positional_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (segment_embedding): Embedding(2, 512)\n",
       "    (segment_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_block): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff19619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
