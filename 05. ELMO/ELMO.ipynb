{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELMO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCCqBtqU65Gb"
      },
      "source": [
        "%%capture\n",
        "!pip install Korpora\n",
        "# !pip install python-mecab-ko\n",
        "\n",
        "from Korpora import Korpora\n",
        "Korpora.fetch(\"namuwikitext\", root_dir='/content')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56CgMd4i688Z"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau, StepLR, LambdaLR\n",
        "\n",
        "import torchtext\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.legacy.data import Dataset, Field, BucketIterator\n",
        "    \n",
        "import math\n",
        "import time\n",
        "import mecab\n",
        "import random\n",
        "import linecache\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "SEED = 1234\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKhwbN-W0UNd"
      },
      "source": [
        "# train_df = pd.read_table(\"namuwikitext/namuwikitext_20200302.test\", header=None, sep=\"\\t\")\n",
        "\n",
        "# tokenizer = mecab.MeCab()\n",
        "\n",
        "# TEXT = Field(\n",
        "#     sequential=True,\n",
        "#     use_vocab=True,\n",
        "#     tokenize=tokenizer.morphs,\n",
        "#     init_token='<sos>',\n",
        "#     eos_token='<eos>',\n",
        "#     unk_token='<unk>',\n",
        "#     pad_token='<pad>',\n",
        "#     lower=True, \n",
        "#     batch_first=True\n",
        "#     ) \n",
        "\n",
        "# train_ds = DataFrameDataset.splits(\n",
        "#   text_field=TEXT, train_df=train_df)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvusiesDwsZX"
      },
      "source": [
        "# # train_data, test_data = TabularDataset.splits(\n",
        "# #     path='drive/MyDrive/', \n",
        "# #     train='train_data_cleaned.txt',\n",
        "# #     test='test_data_cleaned.txt',\n",
        "# #     format='tsv', \n",
        "# #     fields=[('text', TEXT)]\n",
        "# #     )\n",
        "\n",
        "# # train_data = TabularDataset(path='drive/MyDrive/sample_text.csv',\n",
        "# #                             format='csv',\n",
        "# #                             fields=[('text', TEXT)])\n",
        "\n",
        "# train_data = TabularDataset(path='/content/namuwikitext/namuwikitext_20200302.test',\n",
        "#                             format='tsv',\n",
        "#                             fields=[('text', TEXT)])\n",
        "\n",
        "# TEXT.build_vocab(train_data)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffmPH4Sw2E-r"
      },
      "source": [
        "def get_special_list(first, pad, tot_len):\n",
        "    ret = [first]\n",
        "    ret = ret + [pad for _ in range(tot_len-1)]\n",
        "    return ret\n",
        "\n",
        "\n",
        "class KoDataset(Dataset):\n",
        "    def __init__(self, data_path, max_character_length = 10, max_character_size=2000, max_vocab_size = 10000):\n",
        "        \"\"\"\n",
        "        data_paths = list of de path, en data path\n",
        "        \"\"\"\n",
        "        self.ko_path = data_path \n",
        "        self.ko_vocab = Vocabs(tokenizer = 'mecab')\n",
        "        self.ko_vocab.update_vocabs_to_file(self.ko_path)\n",
        "        self.ko_vocab.set_most_common_dict(size=max_vocab_size)\n",
        "\n",
        "        self.id2word_dict = self.ko_vocab.get_index_dict()\n",
        "        self.character_dict = self.ko_vocab.get_character_dict(size = max_character_size)\n",
        "        self.max_character_length = max_character_length\n",
        "\n",
        "        with open(self.ko_path, \"r\") as f:\n",
        "            self._total_data = len(f.readlines())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._total_data \n",
        "\n",
        "    def __getitem__(self, idx):     \n",
        "        raw_ko = linecache.getline(self.ko_path, idx + 1).strip()\n",
        "        ko_tensor_ = torch.tensor([2]+[self.ko_vocab.vocab_dict[token] for token in self.ko_vocab.tokenizer(raw_ko)]+[3]).long()\n",
        "        return ko_tensor_\n",
        "    \n",
        "    def collate_fn(self, data_batch, pad_idx=0, sos_idx=2, eos_idx=3):\n",
        "        ko_batch = []\n",
        "        char_batch = []\n",
        "\n",
        "        token_max_len = self.max_character_length\n",
        "        max_seq_len = 0\n",
        "        for each_item in data_batch:\n",
        "            # token_max_len = max(token_max_len, max([len(self.id2word_dict[int(i)]) for i in each_item]))\n",
        "            max_seq_len = max(max_seq_len, len(each_item))\n",
        "        \n",
        "        for each_item in data_batch:\n",
        "            ko_batch.append(each_item)\n",
        "            chars = []\n",
        "            \n",
        "            for index in each_item:\n",
        "                if index in [sos_idx, eos_idx]:\n",
        "                    padded_each_characters = get_special_list(int(index), pad_idx, token_max_len)\n",
        "                else:\n",
        "                    # 안녕 -> padded_each_characters(index_안 + index_녕 + index_pad)\n",
        "                    padded_each_characters = []\n",
        "                    word = self.id2word_dict[int(index)]\n",
        "                    for char in word:\n",
        "                        padded_each_characters.append(self.character_dict[char])\n",
        "                    if len(padded_each_characters) > token_max_len:\n",
        "                        padded_each_characters = padded_each_characters[:token_max_len]\n",
        "                    else:\n",
        "                        left_length = token_max_len-len(padded_each_characters)\n",
        "                        padded_each_characters = padded_each_characters + [0]*left_length\n",
        "                chars.append(padded_each_characters)\n",
        "\n",
        "            # max_sequence_length padding\n",
        "            chars = chars + [[0 for _ in range(token_max_len)] for _ in range(max_seq_len - len(chars))]\n",
        "            char_batch.append(chars)\n",
        "\n",
        "        padded_ko_index_batch = pad_sequence(ko_batch, padding_value=pad_idx, batch_first=True)\n",
        "        padded_ko_char_batch = torch.Tensor(char_batch).long()\n",
        "        return padded_ko_index_batch, padded_ko_char_batch"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOtvpgJH2FVI"
      },
      "source": [
        "class Vocabs:\n",
        "    def __init__(self, tokenizer=None):\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = lambda x: x.split(\" \")\n",
        "        elif tokenizer == \"mecab\":\n",
        "            self.tokenizer = mecab.MeCab().morphs\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "        self.index_dict = None\n",
        "\n",
        "        self.pad_idx = 0\n",
        "        self.unk_idx = 1\n",
        "        self.sos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "        self._index = 4\n",
        "        self.vocab_dict = defaultdict(lambda: self.unk_idx)\n",
        "        self.vocab_dict[\"<PAD>\"] = self.pad_idx\n",
        "        self.vocab_dict[\"<UNK>\"] = self.unk_idx\n",
        "        self.vocab_dict[\"<SOS>\"] = self.sos_idx\n",
        "        self.vocab_dict[\"<EOS>\"] = self.eos_idx\n",
        "\n",
        "        self.character_dict = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab_dict)\n",
        "\n",
        "\n",
        "    def build_character_dict(self, size = 2000):\n",
        "        char_cnt_dict = defaultdict(lambda:0)\n",
        "\n",
        "        ret = defaultdict(lambda: self.unk_idx)\n",
        "        ret[\"<PAD>\"] = self.pad_idx\n",
        "        ret[\"<UNK>\"] = self.unk_idx\n",
        "        ret[\"<SOS>\"] = self.sos_idx\n",
        "        ret[\"<EOS>\"] = self.eos_idx\n",
        "        _index = 4\n",
        "        for i in self.vocab_dict:\n",
        "            if i not in ['<PAD>','<UNK>','<SOS>','<EOS>']:\n",
        "                for char in i:\n",
        "                    char_cnt_dict[char] += 1\n",
        "\n",
        "        for k, v in Counter(char_cnt_dict).most_common(size):\n",
        "            ret[k] = _index\n",
        "            _index += 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_character_dict(self, size = 2000):\n",
        "        if self.character_dict is None:\n",
        "            self.character_dict = self.build_character_dict(size = size)\n",
        "        return self.character_dict\n",
        "\n",
        "    def set_most_common_dict(self, size):\n",
        "        new_dict = defaultdict(lambda: self.unk_idx)\n",
        "        new_dict[\"<PAD>\"] = self.pad_idx\n",
        "        new_dict[\"<UNK>\"] = self.unk_idx\n",
        "        new_dict[\"<SOS>\"] = self.sos_idx\n",
        "        new_dict[\"<EOS>\"] = self.eos_idx\n",
        "        _index = 4\n",
        "        for k, v in Counter(self.count_dict).most_common(size):\n",
        "            new_dict[k] = _index\n",
        "            _index += 1\n",
        "        self.vocab_dict = new_dict\n",
        "\n",
        "    def update_vocabs_to_file(self, filepath):\n",
        "        count_dict = defaultdict(lambda: 1)\n",
        "        with open(filepath, encoding=\"utf8\") as f:\n",
        "            for string_ in f:\n",
        "                for token in self.tokenizer(string_.replace(\"\\n\",\"\").lower()):\n",
        "                    if token in self.vocab_dict:\n",
        "                        count_dict[token] += 1\n",
        "                        pass\n",
        "                    else:\n",
        "                        count_dict[token] = 1\n",
        "                        self.vocab_dict[token] = self._index\n",
        "                        self._index += 1\n",
        "        self.count_dict = count_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab_dict)\n",
        "\n",
        "    def build_vocabs(self, sentence_list):\n",
        "        for sentence in sentence_list:\n",
        "            tokens_list = self.tokenizer(sentence)\n",
        "            for word in tokens_list:\n",
        "                if word in self.vocab_dict:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.vocab_dict[word] = self._index\n",
        "                    self._index += 1\n",
        "\n",
        "    def build_index_dict(self):\n",
        "        self.index_dict = {v: k for k, v in self.vocab_dict.items()}\n",
        "\n",
        "    def get_index_dict(self):\n",
        "        if self.index_dict == None:\n",
        "            self.build_index_dict()\n",
        "        return self.index_dict\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPiUDu8EzWFE"
      },
      "source": [
        "TrainDataset = KoDataset(\"drive/MyDrive/sample_text.csv\", \n",
        "            max_character_length = 5,\n",
        "            max_character_size = 1000,\n",
        "            max_vocab_size = 10000\n",
        "            )\n",
        "\n",
        "TrainDataloader = DataLoader(TrainDataset, batch_size=2, shuffle=True, collate_fn=TrainDataset.collate_fn)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lamc50oq6oX"
      },
      "source": [
        "class CNN1d(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 pad_idx, dropout=0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = embedding_dim, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = fs)\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):       \n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "        \n",
        "        return self.fc(cat)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVZ9FLtB8JZ7"
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    def __init__(self, size, n_layers, f):\n",
        "        super(Highway, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
        "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
        "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
        "        self.f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in range(self.n_layers):\n",
        "            gate = F.sigmoid(self.gate[layer](x))\n",
        "\n",
        "            nonlinear = self.f(self.nonlinear[layer](x))\n",
        "            linear = self.linear[layer](x)\n",
        "\n",
        "            x = gate * nonlinear + (1 - gate) * linear\n",
        "\n",
        "        return x"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbpEzdl47NaS"
      },
      "source": [
        "class ELMO_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, output_dim, pad_idx, n_layers=2, bidirectional=True):\n",
        "        super().__init__()\n",
        "\n",
        "        n_filters = 100\n",
        "        filter_sizes = [3, 4, 5]\n",
        "\n",
        "        self.embedding = CNN1d(vocab_size, emb_dim, n_filters, filter_sizes, emb_dim, pad_idx)\n",
        "        self.highway   = Highway(size=emb_dim, n_layers=1, f=F.relu)\n",
        "        self.rnn       = nn.LSTM(emb_dim, hid_dim, n_layers, bidirectional=bidirectional)        \n",
        "        self.fc_out    = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedding               = self.embedding(src)\n",
        "        highway                 = self.highway(embedding)\n",
        "\n",
        "        print(highway.shape)\n",
        "\n",
        "        output, (hidden, state) = self.rnn(highway)\n",
        "\n",
        "        batch_size, seq_len, _  = output.size()\n",
        "        output                  = output.reshape(batch_size, seq_len, -1, 2)\n",
        "\n",
        "        forward_hid, backward_hid = output[:, :, :, 0], output[:, :, :, 1]\n",
        "        \n",
        "        # parameter sharing?\n",
        "        forward_pred  = self.fc_out(forward_hid)\n",
        "        backward_pred = self.fc_out(backward_hid)\n",
        "\n",
        "        return forward_pred, backward_pred"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cx-xa3NGgzz"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, output_dim, clip=1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.text\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        fpred, bpred = model(src)\n",
        "\n",
        "        forward_loss = criterion(fpred.reshape(-1, output_dim), trg.reshape(-1))\n",
        "        backward_loss = criterion(bpred.reshape(-1, output_dim), trg.reshape(-1))\n",
        "        loss = forward_loss + backward_loss\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxeIIvIBuYpu"
      },
      "source": [
        "IN_DIM  = len(TrainDataloader.dataset.ko_vocab.vocab_dict)\n",
        "OUT_DIM = len(TrainDataloader.dataset.ko_vocab.vocab_dict)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 1024\n",
        "PAD_IDX = TrainDataloader.dataset.ko_vocab.pad_idx\n",
        "\n",
        "model = ELMO_Embedding(IN_DIM, EMB_DIM, HID_DIM, OUT_DIM, PAD_IDX, n_layers=2, bidirectional=True)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3B2CRmblLiMx",
        "outputId": "9df8d05b-96af-4ed2-d7cd-27a24200fd36"
      },
      "source": [
        "import warnings\n",
        "\n",
        "N_EPOCHS = 2\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "optimizer.zero_grad()\n",
        "optimizer.step()\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, OUT_DIM)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-98d9c8a11554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-4c4e5864c910>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, output_dim, clip)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mfpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mforward_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-151-6638f92cf00c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0membedding\u001b[0m               \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mhighway\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-149-8b3dc91f06fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (300x1 and 300x256)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXyeJ59iL6vA"
      },
      "source": [
        ""
      ],
      "execution_count": 133,
      "outputs": []
    }
  ]
}