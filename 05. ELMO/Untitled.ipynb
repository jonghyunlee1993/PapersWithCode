{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau, StepLR, LambdaLR\n",
    "    \n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 256\n",
    "MAX_WORD_LENGTH_IN_SENT = 25\n",
    "MAX_CHAR_LENGTH_IN_WORD = 6\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earlier-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "from torchtext import data, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_source_and_target(lines, split_cond, fpath=\"data\"):\n",
    "    src = []\n",
    "    trg = []\n",
    "\n",
    "    for line in lines:\n",
    "        src.append(' '.join(line[:-1]) + '\\n')\n",
    "        trg.append(' '.join(line[1:]) + '\\n')\n",
    "    \n",
    "    write_txt(split_cond + \".src\", src, fpath)\n",
    "    write_txt(split_cond + \".trg\", trg, fpath)\n",
    "    \n",
    "def write_txt(fname, lines, fpath):\n",
    "    with open(os.path.join(fpath, fname), \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "if not os.path.exists(\"data/train.src\"):\n",
    "    with open(\"data/petitions_splited_mecab.txt\", \"r\") as f:\n",
    "         corpus = f.readlines()\n",
    "\n",
    "    corpus = list(map(lambda x: str(x).replace(\"\\n\", \"\"), corpus))\n",
    "\n",
    "    train_lines, test_lines = train_test_split(corpus, test_size=0.05, random_state=1234)\n",
    "    train_lines, valid_lines = train_test_split(train_lines, test_size=1/19, random_state=1234)\n",
    "\n",
    "    generate_source_and_target(train_lines, \"train\", fpath=\"data\")\n",
    "    generate_source_and_target(valid_lines, \"val\", fpath=\"data\")\n",
    "    generate_source_and_target(test_lines, \"test\", fpath=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sweet-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source vocabulary: 1610\n",
      "Unique tokens in target vocabulary: 1602\n",
      "number of training data : 205654\n",
      "number of valid data : 11426\n",
      "number of test data : 11426\n"
     ]
    }
   ],
   "source": [
    "class ELMODataset:\n",
    "    def __init__(self, filepath, batch_size, max_length, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.SRC = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                              init_token='<sos>',\n",
    "                              eos_token='<eos>',\n",
    "                              pad_token='<pad>',\n",
    "                              lower=True,\n",
    "                              batch_first=True,\n",
    "                              include_lengths=False,\n",
    "                              fix_length=max_length)\n",
    "        \n",
    "        self.TRG = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                              init_token='<sos>',\n",
    "                              eos_token='<eos>',\n",
    "                              pad_token='<pad>',\n",
    "                              lower=True,\n",
    "                              batch_first=True,\n",
    "                              fix_length=max_length)\n",
    "\n",
    "        self.train_data, self.valid_data, self.test_data = \\\n",
    "            datasets.TranslationDataset.splits(path=filepath, exts=('.src', '.trg'),\n",
    "                                               fields=(self.SRC, self.TRG))\n",
    "\n",
    "        self.build_vocab()\n",
    "\n",
    "        print('number of training data : {}'.format(len(self.train_data)))\n",
    "        print('number of valid data : {}'.format(len(self.valid_data)))\n",
    "        print('number of test data : {}'.format(len(self.test_data)))\n",
    "\n",
    "        self.train_iterator, self.valid_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (self.train_data, self.valid_data, self.test_data), sort=True, sort_within_batch=True,\n",
    "            batch_size=self.batch_size, device=self.device)\n",
    " \n",
    "    def build_vocab(self, min_freq=5):\n",
    "        self.SRC.build_vocab(self.train_data, min_freq=min_freq)\n",
    "        self.TRG.build_vocab(self.train_data, min_freq=min_freq)\n",
    "        \n",
    "        print(f\"Unique tokens in source vocabulary: {len(self.SRC.vocab)}\")\n",
    "        print(f\"Unique tokens in target vocabulary: {len(self.TRG.vocab)}\")\n",
    "\n",
    "elmo_dataset = ELMODataset(filepath=\"data\", batch_size=BATCH_SIZE, max_length=MAX_WORD_LENGTH_IN_SENT, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "naval-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDecomposer:\n",
    "    def __init__(self, elmo_dataset, max_word_in_sent, max_char_in_word, special_token_idx=[0, 1, 2, 3]):\n",
    "        self.elmo_dataset = elmo_dataset\n",
    "        self.max_word_in_sent = max_word_in_sent\n",
    "        self.max_char_in_word = max_char_in_word\n",
    "        self.special_token_idx = special_token_idx\n",
    "        \n",
    "        self.build_char_vocab()\n",
    "        \n",
    "    def build_char_vocab(self):\n",
    "        char_vocab = set([char for word in self.elmo_dataset.SRC.vocab.itos for char in word])\n",
    "        self.ctoi = {}\n",
    "        self.itoc = {}\n",
    "        \n",
    "        for idx, char in enumerate(char_vocab):\n",
    "            self.ctoi[char] = idx\n",
    "            self.itoc[idx]  = char\n",
    "            \n",
    "    def decompose(self, src):\n",
    "        # pad token이 1로 되어 있음 주의\n",
    "        batch_char_embedding = np.ones((src.shape[0], self.max_word_in_sent, self.max_char_in_word)).astype(int)\n",
    "        \n",
    "        for batch_order_idx, sent in enumerate(src):\n",
    "            for word_order_idx, s in enumerate(sent):\n",
    "                if word_order_idx < self.max_word_in_sent - 1:\n",
    "                    if s in self.special_token_idx:\n",
    "                        batch_char_embedding[batch_order_idx, word_order_idx, 0] = s\n",
    "#                         if s == 0:\n",
    "#                             # unk token\n",
    "#                             batch_char_embedding[batch_order_idx, word_order_idx, 0] = 1\n",
    "#                         elif s == 2 or s == 3:\n",
    "#                             batch_char_embedding[batch_order_idx, word_order_idx, 0] = s\n",
    "                    elif s not in self.special_token_idx:\n",
    "                        for char_order_idx, char in enumerate(self.elmo_dataset.SRC.vocab.itos[s]):\n",
    "                            if char_order_idx < self.max_char_in_word - 1:\n",
    "                                batch_char_embedding[batch_order_idx, word_order_idx, char_order_idx] = self.ctoi[char]\n",
    "                                \n",
    "                                                             \n",
    "        return torch.LongTensor(batch_char_embedding)\n",
    "    \n",
    "character_decomposer = CharacterDecomposer(elmo_dataset, max_word_in_sent=MAX_WORD_LENGTH_IN_SENT, max_char_in_word=MAX_CHAR_LENGTH_IN_WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cosmetic-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in title vocabulary: 1610\n",
      "Unique tokens in label vocabulary: 9\n",
      "number of training data : 45678\n",
      "number of valid data : 9107\n",
      "number of test data : 10\n"
     ]
    }
   ],
   "source": [
    "class CLSDataset:\n",
    "    def __init__(self, elmo_dataset, filepath, batch_size, max_length, device):\n",
    "        self.elmo_dataset = elmo_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.title = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                                init_token='<sos>',\n",
    "                                eos_token='<eos>',\n",
    "                                pad_token='<pad>',\n",
    "                                lower=True,\n",
    "                                batch_first=True,\n",
    "                                include_lengths=False, \n",
    "                                fix_length=max_length)\n",
    "        \n",
    "        self.label = data.Field(lower=True,\n",
    "                                batch_first=True)\n",
    "        \n",
    "        fields = [('label', self.label), ('title', self.title)]\n",
    "        self.train_data, self.valid_data, self.test_data = data.TabularDataset.splits(path=filepath,\n",
    "                                                                                      train='train_tokenized.ynat',\n",
    "                                                                                      validation='val_tokenized.ynat',\n",
    "                                                                                      test='test_tokenized.ynat',\n",
    "                                                                                      format='tsv',\n",
    "                                                                                      fields=fields)\n",
    "        \n",
    "        self.build_vocab()\n",
    "        print('number of training data : {}'.format(len(self.train_data)))\n",
    "        print('number of valid data : {}'.format(len(self.valid_data)))\n",
    "        print('number of test data : {}'.format(len(self.test_data)))\n",
    "        \n",
    "        self.train_iterator, self.valid_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (self.train_data, self.valid_data, self.test_data), sort=True, sort_within_batch=True,\n",
    "            batch_size=self.batch_size, device=self.device, sort_key=lambda x: len(x.title))\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        self.title.build_vocab(self.train_data, min_freq=1)\n",
    "        self.title.vocab = self.elmo_dataset.SRC.vocab\n",
    "        self.label.build_vocab(self.train_data, min_freq=1)\n",
    "        \n",
    "        print(f\"Unique tokens in title vocabulary: {len(self.title.vocab)}\")\n",
    "        print(f\"Unique tokens in label vocabulary: {len(self.label.vocab)}\")\n",
    "        \n",
    "cls_dataset = CLSDataset(elmo_dataset, filepath=\"data\", batch_size=BATCH_SIZE, max_length=MAX_WORD_LENGTH_IN_SENT, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "inside-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, pad_idx, dropout=0.2):\n",
    "        super(CNN1d, self).__init__()   \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels  = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size  = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):       \n",
    "        embedded = self.embedding(src)\n",
    "        batch_size, word_len, char_len, emb_dim = embedded.size()\n",
    "        \n",
    "        # [batch * word_len, char_len, emb_dim]\n",
    "        embedded = embedded.reshape(-1, char_len, emb_dim)  \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat    = self.dropout(torch.cat(pooled, dim=1))\n",
    "        \n",
    "        output = self.fc(cat)\n",
    "        output = output.reshape(batch_size, word_len, -1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, n_layers, f):\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in range(self.n_layers):\n",
    "            gate = F.sigmoid(self.gate[layer](x))\n",
    "\n",
    "            nonlinear = self.f(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "\n",
    "        return x\n",
    "    \n",
    "class ELMO_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, n_layers=2, bidirectional=True):\n",
    "        super(ELMO_Embedding, self).__init__()\n",
    "\n",
    "        n_filters = 100\n",
    "        filter_sizes = [3, 4, 5]\n",
    "\n",
    "        self.embedding = CNN1d(vocab_size, embedding_dim, n_filters, filter_sizes, embedding_dim, pad_idx)\n",
    "        self.highway   = Highway(size=embedding_dim, n_layers=1, f=F.relu)\n",
    "        self.rnn       = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional, batch_first=True)        \n",
    "        self.fc_out    = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, max_word_len=20):\n",
    "        hiddens = []\n",
    "        embeddings = []\n",
    "        steps = max_word_len\n",
    "        \n",
    "        for step in range(steps):\n",
    "            inputs = src[:, i, :].unsqueeze(1)\n",
    "            embedding = self.highway(self.embedding(inputs))\n",
    "            output, (hidden, state) = self.rnn(embedding)\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "            hiddens.append(hidden)\n",
    "\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        return embeddings, hiddens\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-seeking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "affiliated-retail",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,   1,   1,   1,   1,   1],\n",
      "        [  0,   1,   1,   1,   1,   1],\n",
      "        [831,   1,   1,   1,   1,   1],\n",
      "        [  0,   1,   1,   1,   1,   1],\n",
      "        [  0,   1,   1,   1,   1,   1],\n",
      "        [  3,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1],\n",
      "        [  1,   1,   1,   1,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "for batch in cls_dataset.train_iterator: \n",
    "    title = character_decomposer.decompose(batch.title)\n",
    "\n",
    "    print(title[25])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-remark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
