{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imposed-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau, StepLR, LambdaLR\n",
    "    \n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 256\n",
    "MAX_WORD_LENGTH_IN_SENT = 25\n",
    "MAX_CHAR_LENGTH_IN_WORD = 6\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "useful-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "from torchtext import data, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_source_and_target(lines, split_cond, fpath=\"data\"):\n",
    "    src = []\n",
    "    trg = []\n",
    "\n",
    "    for line in lines:\n",
    "        src.append(line[:-1] + '\\n')\n",
    "        trg.append(line[1:] + '\\n')\n",
    "    \n",
    "    write_txt(split_cond + \".src\", src, fpath)\n",
    "    write_txt(split_cond + \".trg\", trg, fpath)\n",
    "    \n",
    "def write_txt(fname, lines, fpath):\n",
    "    with open(os.path.join(fpath, fname), \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "if not os.path.exists(\"data/train.src\"):\n",
    "    with open(\"data/petitions_splited_mecab.txt\", \"r\") as f:\n",
    "         corpus = f.readlines()\n",
    "\n",
    "    corpus = list(map(lambda x: str(x).replace(\"\\n\", \"\"), corpus))\n",
    "\n",
    "    train_lines, test_lines = train_test_split(corpus, test_size=0.05, random_state=1234)\n",
    "    train_lines, valid_lines = train_test_split(train_lines, test_size=1/19, random_state=1234)\n",
    "\n",
    "    generate_source_and_target(train_lines, \"train\", fpath=\"data\")\n",
    "    generate_source_and_target(valid_lines, \"val\", fpath=\"data\")\n",
    "    generate_source_and_target(test_lines, \"test\", fpath=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "velvet-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source vocabulary: 1610\n",
      "Unique tokens in target vocabulary: 1602\n",
      "number of training data : 205654\n",
      "number of valid data : 11426\n",
      "number of test data : 11426\n"
     ]
    }
   ],
   "source": [
    "class ELMODataset:\n",
    "    def __init__(self, filepath, batch_size, max_length, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.SRC = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                              init_token='<sos>',\n",
    "                              eos_token='<eos>',\n",
    "                              pad_token='<pad>',\n",
    "                              lower=True,\n",
    "                              batch_first=True,\n",
    "                              include_lengths=False,\n",
    "                              fix_length=max_length)\n",
    "        \n",
    "        self.TRG = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                              init_token='<sos>',\n",
    "                              eos_token='<eos>',\n",
    "                              pad_token='<pad>',\n",
    "                              lower=True,\n",
    "                              batch_first=True,\n",
    "                              fix_length=max_length)\n",
    "\n",
    "        self.train_data, self.valid_data, self.test_data = \\\n",
    "            datasets.TranslationDataset.splits(path=filepath, exts=('.src', '.trg'),\n",
    "                                               fields=(self.SRC, self.TRG))\n",
    "\n",
    "        self.build_vocab()\n",
    "\n",
    "        print('number of training data : {}'.format(len(self.train_data)))\n",
    "        print('number of valid data : {}'.format(len(self.valid_data)))\n",
    "        print('number of test data : {}'.format(len(self.test_data)))\n",
    "\n",
    "        self.train_iterator, self.valid_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (self.train_data, self.valid_data, self.test_data), sort=True, sort_within_batch=True,\n",
    "            batch_size=self.batch_size, device=self.device)\n",
    " \n",
    "    def build_vocab(self, min_freq=5):\n",
    "        self.SRC.build_vocab(self.train_data, min_freq=min_freq)\n",
    "        self.TRG.build_vocab(self.train_data, min_freq=min_freq)\n",
    "        \n",
    "        print(f\"Unique tokens in source vocabulary: {len(self.SRC.vocab)}\")\n",
    "        print(f\"Unique tokens in target vocabulary: {len(self.TRG.vocab)}\")\n",
    "\n",
    "elmo_dataset = ELMODataset(filepath=\"data\", batch_size=BATCH_SIZE, max_length=MAX_WORD_LENGTH_IN_SENT, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "removable-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDecomposer:\n",
    "    def __init__(self, elmo_dataset, max_word_in_sent, max_char_in_word, special_token_idx=[0, 1, 2, 3]):\n",
    "        self.elmo_dataset = elmo_dataset\n",
    "        self.max_word_in_sent = max_word_in_sent\n",
    "        self.max_char_in_word = max_char_in_word\n",
    "        self.special_token_idx = special_token_idx\n",
    "        \n",
    "        self.build_char_vocab()\n",
    "        \n",
    "    def build_char_vocab(self):\n",
    "        char_vocab = set([char for word in self.elmo_dataset.SRC.vocab.itos for char in word])\n",
    "        self.ctoi = {}\n",
    "        self.itoc = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx + 4, char in enumerate(char_vocab):\n",
    "            self.ctoi[char] = idx\n",
    "            self.itoc[idx]  = char\n",
    "            \n",
    "    def decompose(self, src):\n",
    "        # pad token이 1로 되어 있음 주의\n",
    "        batch_char_embedding = np.ones((src.shape[0], self.max_word_in_sent, self.max_char_in_word)).astype(int)\n",
    "        \n",
    "        for batch_order_idx, sent in enumerate(src):\n",
    "            for word_order_idx, s in enumerate(sent):\n",
    "                if word_order_idx < self.max_word_in_sent - 1:\n",
    "                    if s in self.special_token_idx:\n",
    "                        batch_char_embedding[batch_order_idx, word_order_idx, 0] = s\n",
    "#                         if s == 0:\n",
    "#                             # unk token\n",
    "#                             batch_char_embedding[batch_order_idx, word_order_idx, 0] = 1\n",
    "#                         elif s == 2 or s == 3:\n",
    "#                             batch_char_embedding[batch_order_idx, word_order_idx, 0] = s\n",
    "                    elif s not in self.special_token_idx:\n",
    "                        for char_order_idx, char in enumerate(self.elmo_dataset.SRC.vocab.itos[s]):\n",
    "                            if char_order_idx < self.max_char_in_word - 1:\n",
    "                                batch_char_embedding[batch_order_idx, word_order_idx, char_order_idx] = self.ctoi[char] + 4\n",
    "                                                                                    \n",
    "        return torch.LongTensor(batch_char_embedding)\n",
    "    \n",
    "character_decomposer = CharacterDecomposer(elmo_dataset, max_word_in_sent=MAX_WORD_LENGTH_IN_SENT, max_char_in_word=MAX_CHAR_LENGTH_IN_WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "political-bahrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elmo_dataset.SRC.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "involved-portuguese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'과': 0,\n",
       " '규': 1,\n",
       " '럿': 2,\n",
       " '했': 3,\n",
       " '론': 4,\n",
       " '썽': 5,\n",
       " '딩': 6,\n",
       " '결': 7,\n",
       " '?': 8,\n",
       " '갰': 9,\n",
       " '섣': 10,\n",
       " '쩍': 11,\n",
       " '냔': 12,\n",
       " '텋': 13,\n",
       " '돕': 14,\n",
       " '텼': 15,\n",
       " '선': 16,\n",
       " '갚': 17,\n",
       " '째': 18,\n",
       " '츠': 19,\n",
       " '오': 20,\n",
       " '뎅': 21,\n",
       " '돼': 22,\n",
       " '딥': 23,\n",
       " '탠': 24,\n",
       " '뼛': 25,\n",
       " '려': 26,\n",
       " '써': 27,\n",
       " '템': 28,\n",
       " '웹': 29,\n",
       " '꿉': 30,\n",
       " '징': 31,\n",
       " '얏': 32,\n",
       " '환': 33,\n",
       " '팁': 34,\n",
       " '뻣': 35,\n",
       " '찔': 36,\n",
       " '짙': 37,\n",
       " '캄': 38,\n",
       " '거': 39,\n",
       " '허': 40,\n",
       " '잃': 41,\n",
       " '비': 42,\n",
       " '빵': 43,\n",
       " '잣': 44,\n",
       " '씀': 45,\n",
       " '집': 46,\n",
       " '혓': 47,\n",
       " '삿': 48,\n",
       " '쩌': 49,\n",
       " '섞': 50,\n",
       " '윗': 51,\n",
       " '잏': 52,\n",
       " '켓': 53,\n",
       " '샅': 54,\n",
       " '숭': 55,\n",
       " '팟': 56,\n",
       " '대': 57,\n",
       " '것': 58,\n",
       " '쵸': 59,\n",
       " '혜': 60,\n",
       " '휼': 61,\n",
       " '삽': 62,\n",
       " '웁': 63,\n",
       " '레': 64,\n",
       " '빈': 65,\n",
       " '휠': 66,\n",
       " '꿰': 67,\n",
       " '컴': 68,\n",
       " '짊': 69,\n",
       " '꿈': 70,\n",
       " '택': 71,\n",
       " '투': 72,\n",
       " '행': 73,\n",
       " '누': 74,\n",
       " '쪼': 75,\n",
       " '젼': 76,\n",
       " '톡': 77,\n",
       " '낌': 78,\n",
       " '死': 79,\n",
       " '압': 80,\n",
       " 'z': 81,\n",
       " '긁': 82,\n",
       " '든': 83,\n",
       " '빼': 84,\n",
       " '죗': 85,\n",
       " '원': 86,\n",
       " '컹': 87,\n",
       " '낱': 88,\n",
       " '액': 89,\n",
       " '척': 90,\n",
       " '헤': 91,\n",
       " '입': 92,\n",
       " '싸': 93,\n",
       " '끙': 94,\n",
       " '낫': 95,\n",
       " '졍': 96,\n",
       " '6': 97,\n",
       " '껍': 98,\n",
       " '깍': 99,\n",
       " '썩': 100,\n",
       " '델': 101,\n",
       " '漢': 102,\n",
       " '떄': 103,\n",
       " '셍': 104,\n",
       " '딨': 105,\n",
       " '컷': 106,\n",
       " '당': 107,\n",
       " '새': 108,\n",
       " '탈': 109,\n",
       " '쉬': 110,\n",
       " '맥': 111,\n",
       " '뺏': 112,\n",
       " '겄': 113,\n",
       " '란': 114,\n",
       " '흘': 115,\n",
       " '몹': 116,\n",
       " '낯': 117,\n",
       " '꽉': 118,\n",
       " '협': 119,\n",
       " '껴': 120,\n",
       " '섰': 121,\n",
       " '팎': 122,\n",
       " '굿': 123,\n",
       " '찼': 124,\n",
       " '께': 125,\n",
       " '꾸': 126,\n",
       " '有': 127,\n",
       " '찟': 128,\n",
       " 'r': 129,\n",
       " '핀': 130,\n",
       " '5': 131,\n",
       " '곽': 132,\n",
       " '솔': 133,\n",
       " '첵': 134,\n",
       " '퀸': 135,\n",
       " '옷': 136,\n",
       " '땐': 137,\n",
       " '꾹': 138,\n",
       " 'w': 139,\n",
       " '봣': 140,\n",
       " '쉼': 141,\n",
       " '홧': 142,\n",
       " '롤': 143,\n",
       " '폭': 144,\n",
       " '쌉': 145,\n",
       " '쥬': 146,\n",
       " '튜': 147,\n",
       " '휀': 148,\n",
       " '직': 149,\n",
       " '양': 150,\n",
       " '몾': 151,\n",
       " '흙': 152,\n",
       " '신': 153,\n",
       " '헹': 154,\n",
       " '게': 155,\n",
       " '걸': 156,\n",
       " '밀': 157,\n",
       " '뺑': 158,\n",
       " '드': 159,\n",
       " '씩': 160,\n",
       " '씬': 161,\n",
       " '깔': 162,\n",
       " '홍': 163,\n",
       " '막': 164,\n",
       " '테': 165,\n",
       " '옴': 166,\n",
       " '덫': 167,\n",
       " '쫒': 168,\n",
       " '엾': 169,\n",
       " '땔': 170,\n",
       " '앨': 171,\n",
       " '때': 172,\n",
       " '설': 173,\n",
       " '갤': 174,\n",
       " '뜨': 175,\n",
       " '얕': 176,\n",
       " '핌': 177,\n",
       " '슬': 178,\n",
       " '뭘': 179,\n",
       " '흥': 180,\n",
       " '망': 181,\n",
       " '녀': 182,\n",
       " '폼': 183,\n",
       " 'p': 184,\n",
       " '조': 185,\n",
       " '쏴': 186,\n",
       " '죽': 187,\n",
       " '엇': 188,\n",
       " '쓸': 189,\n",
       " '낟': 190,\n",
       " '쿨': 191,\n",
       " '헙': 192,\n",
       " '국': 193,\n",
       " '옮': 194,\n",
       " '쩐': 195,\n",
       " '꺠': 196,\n",
       " '시': 197,\n",
       " '펼': 198,\n",
       " '논': 199,\n",
       " '셉': 200,\n",
       " '남': 201,\n",
       " '능': 202,\n",
       " '끓': 203,\n",
       " '씁': 204,\n",
       " '불': 205,\n",
       " '뵙': 206,\n",
       " '곡': 207,\n",
       " '팥': 208,\n",
       " '톨': 209,\n",
       " '멧': 210,\n",
       " '훼': 211,\n",
       " '쎄': 212,\n",
       " '낚': 213,\n",
       " '등': 214,\n",
       " '옵': 215,\n",
       " '칙': 216,\n",
       " '딤': 217,\n",
       " '뺨': 218,\n",
       " '뺌': 219,\n",
       " '챙': 220,\n",
       " '욱': 221,\n",
       " '곁': 222,\n",
       " '잦': 223,\n",
       " '눈': 224,\n",
       " '박': 225,\n",
       " '승': 226,\n",
       " '베': 227,\n",
       " '뒤': 228,\n",
       " '니': 229,\n",
       " '라': 230,\n",
       " '며': 231,\n",
       " '금': 232,\n",
       " '디': 233,\n",
       " '공': 234,\n",
       " '깊': 235,\n",
       " '책': 236,\n",
       " '달': 237,\n",
       " '뱃': 238,\n",
       " '잇': 239,\n",
       " '섯': 240,\n",
       " '탄': 241,\n",
       " 's': 242,\n",
       " '숫': 243,\n",
       " '쨉': 244,\n",
       " '냐': 245,\n",
       " '픽': 246,\n",
       " '씨': 247,\n",
       " '밤': 248,\n",
       " '핸': 249,\n",
       " '뺐': 250,\n",
       " '얗': 251,\n",
       " '곰': 252,\n",
       " '캘': 253,\n",
       " '콤': 254,\n",
       " '함': 255,\n",
       " 'e': 256,\n",
       " '독': 257,\n",
       " '빙': 258,\n",
       " '례': 259,\n",
       " '래': 260,\n",
       " '글': 261,\n",
       " '컨': 262,\n",
       " '예': 263,\n",
       " '꺽': 264,\n",
       " '듯': 265,\n",
       " '촘': 266,\n",
       " '놧': 267,\n",
       " '켜': 268,\n",
       " '샵': 269,\n",
       " '겠': 270,\n",
       " '괘': 271,\n",
       " '껑': 272,\n",
       " '솜': 273,\n",
       " '툴': 274,\n",
       " '특': 275,\n",
       " '몸': 276,\n",
       " '붕': 277,\n",
       " '뚝': 278,\n",
       " '촛': 279,\n",
       " '듧': 280,\n",
       " '픕': 281,\n",
       " '팰': 282,\n",
       " '韓': 283,\n",
       " '침': 284,\n",
       " '땠': 285,\n",
       " '쨌': 286,\n",
       " '헷': 287,\n",
       " '태': 288,\n",
       " '여': 289,\n",
       " '쯧': 290,\n",
       " '힙': 291,\n",
       " '뿌': 292,\n",
       " '로': 293,\n",
       " '안': 294,\n",
       " '민': 295,\n",
       " '옅': 296,\n",
       " '렇': 297,\n",
       " '쏘': 298,\n",
       " '줘': 299,\n",
       " '진': 300,\n",
       " '출': 301,\n",
       " '키': 302,\n",
       " 'n': 303,\n",
       " '전': 304,\n",
       " '술': 305,\n",
       " '짓': 306,\n",
       " '볶': 307,\n",
       " '렌': 308,\n",
       " '팀': 309,\n",
       " '흐': 310,\n",
       " '앴': 311,\n",
       " '뼈': 312,\n",
       " '캐': 313,\n",
       " '쇼': 314,\n",
       " '美': 315,\n",
       " '낡': 316,\n",
       " '젊': 317,\n",
       " '됄': 318,\n",
       " '쓴': 319,\n",
       " '평': 320,\n",
       " '딛': 321,\n",
       " 'f': 322,\n",
       " '솎': 323,\n",
       " '찡': 324,\n",
       " '플': 325,\n",
       " '9': 326,\n",
       " '잉': 327,\n",
       " 'm': 328,\n",
       " '뺄': 329,\n",
       " '프': 330,\n",
       " '펜': 331,\n",
       " 'j': 332,\n",
       " '콕': 333,\n",
       " '늑': 334,\n",
       " '령': 335,\n",
       " '헐': 336,\n",
       " '펀': 337,\n",
       " '끔': 338,\n",
       " '풉': 339,\n",
       " '최': 340,\n",
       " '힌': 341,\n",
       " '핵': 342,\n",
       " '떼': 343,\n",
       " '밑': 344,\n",
       " '탕': 345,\n",
       " '편': 346,\n",
       " '굽': 347,\n",
       " '단': 348,\n",
       " '람': 349,\n",
       " '혈': 350,\n",
       " '뿐': 351,\n",
       " '휩': 352,\n",
       " '퓰': 353,\n",
       " '별': 354,\n",
       " '숲': 355,\n",
       " '졸': 356,\n",
       " '겜': 357,\n",
       " '利': 358,\n",
       " '약': 359,\n",
       " '뽀': 360,\n",
       " '쿄': 361,\n",
       " '따': 362,\n",
       " '졌': 363,\n",
       " '픈': 364,\n",
       " '렁': 365,\n",
       " '훌': 366,\n",
       " '기': 367,\n",
       " '운': 368,\n",
       " '탱': 369,\n",
       " '깅': 370,\n",
       " '괜': 371,\n",
       " '떴': 372,\n",
       " '켰': 373,\n",
       " 't': 374,\n",
       " '쪽': 375,\n",
       " '범': 376,\n",
       " '답': 377,\n",
       " '및': 378,\n",
       " '띈': 379,\n",
       " '를': 380,\n",
       " '봉': 381,\n",
       " '믹': 382,\n",
       " '동': 383,\n",
       " '임': 384,\n",
       " '깡': 385,\n",
       " '밌': 386,\n",
       " '젖': 387,\n",
       " '휜': 388,\n",
       " '옹': 389,\n",
       " '!': 390,\n",
       " '곱': 391,\n",
       " '춤': 392,\n",
       " '됀': 393,\n",
       " '띵': 394,\n",
       " '뭡': 395,\n",
       " '걱': 396,\n",
       " '혹': 397,\n",
       " '펑': 398,\n",
       " '랑': 399,\n",
       " '섬': 400,\n",
       " '추': 401,\n",
       " '퐁': 402,\n",
       " '햇': 403,\n",
       " '켐': 404,\n",
       " '룸': 405,\n",
       " '無': 406,\n",
       " '떳': 407,\n",
       " '罪': 408,\n",
       " '됩': 409,\n",
       " '쉴': 410,\n",
       " '월': 411,\n",
       " '을': 412,\n",
       " '她': 413,\n",
       " '퉈': 414,\n",
       " '끽': 415,\n",
       " '긴': 416,\n",
       " '죠': 417,\n",
       " '봇': 418,\n",
       " '싯': 419,\n",
       " '귤': 420,\n",
       " '야': 421,\n",
       " '쩔': 422,\n",
       " '맘': 423,\n",
       " '몰': 424,\n",
       " '춘': 425,\n",
       " '천': 426,\n",
       " '씹': 427,\n",
       " '엠': 428,\n",
       " '닉': 429,\n",
       " '확': 430,\n",
       " '명': 431,\n",
       " '멋': 432,\n",
       " '철': 433,\n",
       " '벨': 434,\n",
       " '퍽': 435,\n",
       " '즐': 436,\n",
       " '뭇': 437,\n",
       " '멤': 438,\n",
       " '겸': 439,\n",
       " '체': 440,\n",
       " '둬': 441,\n",
       " '돗': 442,\n",
       " '음': 443,\n",
       " '힘': 444,\n",
       " '핍': 445,\n",
       " '쿠': 446,\n",
       " '속': 447,\n",
       " '앰': 448,\n",
       " '꺾': 449,\n",
       " '튼': 450,\n",
       " '뉴': 451,\n",
       " '날': 452,\n",
       " '몀': 453,\n",
       " '윤': 454,\n",
       " '7': 455,\n",
       " '빳': 456,\n",
       " '토': 457,\n",
       " '옐': 458,\n",
       " '꼰': 459,\n",
       " '훅': 460,\n",
       " '빤': 461,\n",
       " '찌': 462,\n",
       " '받': 463,\n",
       " '밴': 464,\n",
       " '램': 465,\n",
       " '맡': 466,\n",
       " '벅': 467,\n",
       " '컫': 468,\n",
       " '두': 469,\n",
       " '매': 470,\n",
       " '질': 471,\n",
       " '른': 472,\n",
       " '몆': 473,\n",
       " '콘': 474,\n",
       " '할': 475,\n",
       " '져': 476,\n",
       " '훈': 477,\n",
       " '터': 478,\n",
       " '뒀': 479,\n",
       " '맏': 480,\n",
       " '뺀': 481,\n",
       " '낍': 482,\n",
       " '냈': 483,\n",
       " '웬': 484,\n",
       " '빢': 485,\n",
       " '맑': 486,\n",
       " '엉': 487,\n",
       " '숍': 488,\n",
       " '탑': 489,\n",
       " '펫': 490,\n",
       " '역': 491,\n",
       " '줏': 492,\n",
       " '깽': 493,\n",
       " 'k': 494,\n",
       " '풋': 495,\n",
       " '꼬': 496,\n",
       " '팔': 497,\n",
       " '뛰': 498,\n",
       " '딘': 499,\n",
       " '륵': 500,\n",
       " '첼': 501,\n",
       " '됏': 502,\n",
       " '됨': 503,\n",
       " '샀': 504,\n",
       " '찢': 505,\n",
       " '퀵': 506,\n",
       " '총': 507,\n",
       " '뭄': 508,\n",
       " '생': 509,\n",
       " '츄': 510,\n",
       " '저': 511,\n",
       " '갯': 512,\n",
       " '모': 513,\n",
       " '객': 514,\n",
       " '초': 515,\n",
       " 'b': 516,\n",
       " '겉': 517,\n",
       " '렵': 518,\n",
       " '잖': 519,\n",
       " '뭍': 520,\n",
       " '황': 521,\n",
       " '궤': 522,\n",
       " '급': 523,\n",
       " '뭣': 524,\n",
       " '좀': 525,\n",
       " '손': 526,\n",
       " '륨': 527,\n",
       " '앙': 528,\n",
       " '빅': 529,\n",
       " '준': 530,\n",
       " '욧': 531,\n",
       " '왔': 532,\n",
       " '묵': 533,\n",
       " '탭': 534,\n",
       " '둠': 535,\n",
       " '팽': 536,\n",
       " '엎': 537,\n",
       " '쁜': 538,\n",
       " '굵': 539,\n",
       " '타': 540,\n",
       " '트': 541,\n",
       " '는': 542,\n",
       " '뇨': 543,\n",
       " '횃': 544,\n",
       " '깐': 545,\n",
       " '데': 546,\n",
       " '둡': 547,\n",
       " '밖': 548,\n",
       " '옛': 549,\n",
       " '쌍': 550,\n",
       " '병': 551,\n",
       " '쑈': 552,\n",
       " '떨': 553,\n",
       " '랍': 554,\n",
       " '렷': 555,\n",
       " '깝': 556,\n",
       " '족': 557,\n",
       " '쏙': 558,\n",
       " '년': 559,\n",
       " '꽤': 560,\n",
       " '사': 561,\n",
       " '맹': 562,\n",
       " '0': 563,\n",
       " '틀': 564,\n",
       " '짱': 565,\n",
       " '뽑': 566,\n",
       " '늬': 567,\n",
       " '왜': 568,\n",
       " '꼴': 569,\n",
       " '탐': 570,\n",
       " '귄': 571,\n",
       " '똘': 572,\n",
       " '셜': 573,\n",
       " '앉': 574,\n",
       " '웠': 575,\n",
       " '꼽': 576,\n",
       " '썸': 577,\n",
       " '묶': 578,\n",
       " '표': 579,\n",
       " '영': 580,\n",
       " '죄': 581,\n",
       " '낄': 582,\n",
       " '제': 583,\n",
       " '댓': 584,\n",
       " '엿': 585,\n",
       " '굴': 586,\n",
       " '왕': 587,\n",
       " '크': 588,\n",
       " '률': 589,\n",
       " '쳬': 590,\n",
       " '셰': 591,\n",
       " '뺒': 592,\n",
       " '므': 593,\n",
       " '념': 594,\n",
       " '상': 595,\n",
       " '렘': 596,\n",
       " '찍': 597,\n",
       " '르': 598,\n",
       " '붙': 599,\n",
       " '늠': 600,\n",
       " '폄': 601,\n",
       " '빌': 602,\n",
       " '핏': 603,\n",
       " '닥': 604,\n",
       " '곪': 605,\n",
       " '칼': 606,\n",
       " '랫': 607,\n",
       " '삭': 608,\n",
       " '넬': 609,\n",
       " '엣': 610,\n",
       " '쏟': 611,\n",
       " '뱀': 612,\n",
       " '냇': 613,\n",
       " '이': 614,\n",
       " '몇': 615,\n",
       " '둣': 616,\n",
       " '쌓': 617,\n",
       " '칠': 618,\n",
       " '잊': 619,\n",
       " 'x': 620,\n",
       " '관': 621,\n",
       " '빨': 622,\n",
       " '겨': 623,\n",
       " '짤': 624,\n",
       " '융': 625,\n",
       " '꿋': 626,\n",
       " '뢰': 627,\n",
       " '잘': 628,\n",
       " '밍': 629,\n",
       " '좌': 630,\n",
       " '됬': 631,\n",
       " '햄': 632,\n",
       " '값': 633,\n",
       " '덧': 634,\n",
       " '틸': 635,\n",
       " '멉': 636,\n",
       " '냅': 637,\n",
       " '밋': 638,\n",
       " '풍': 639,\n",
       " '칸': 640,\n",
       " '덩': 641,\n",
       " '심': 642,\n",
       " '샐': 643,\n",
       " '띄': 644,\n",
       " '숨': 645,\n",
       " '삥': 646,\n",
       " '항': 647,\n",
       " '읽': 648,\n",
       " '방': 649,\n",
       " '밣': 650,\n",
       " '벋': 651,\n",
       " '쩨': 652,\n",
       " 'l': 653,\n",
       " '젠': 654,\n",
       " '곳': 655,\n",
       " '팸': 656,\n",
       " '랭': 657,\n",
       " '大': 658,\n",
       " '회': 659,\n",
       " '쫌': 660,\n",
       " '짬': 661,\n",
       " '굼': 662,\n",
       " '말': 663,\n",
       " '측': 664,\n",
       " '츰': 665,\n",
       " '믿': 666,\n",
       " '브': 667,\n",
       " '젝': 668,\n",
       " '럴': 669,\n",
       " '.': 670,\n",
       " '없': 671,\n",
       " '듭': 672,\n",
       " '좋': 673,\n",
       " '혔': 674,\n",
       " '락': 675,\n",
       " '뗀': 676,\n",
       " '찻': 677,\n",
       " '왓': 678,\n",
       " '묘': 679,\n",
       " '몽': 680,\n",
       " '뽕': 681,\n",
       " '넥': 682,\n",
       " ',': 683,\n",
       " '덯': 684,\n",
       " '벗': 685,\n",
       " '벡': 686,\n",
       " '난': 687,\n",
       " '첨': 688,\n",
       " '댜': 689,\n",
       " '쾌': 690,\n",
       " '法': 691,\n",
       " '장': 692,\n",
       " '주': 693,\n",
       " '젤': 694,\n",
       " '곧': 695,\n",
       " '랙': 696,\n",
       " '텝': 697,\n",
       " '쇄': 698,\n",
       " '흰': 699,\n",
       " '줄': 700,\n",
       " '갸': 701,\n",
       " '곘': 702,\n",
       " '호': 703,\n",
       " '빽': 704,\n",
       " '맨': 705,\n",
       " '틈': 706,\n",
       " '괴': 707,\n",
       " '짝': 708,\n",
       " '갓': 709,\n",
       " '깰': 710,\n",
       " '밟': 711,\n",
       " '쇳': 712,\n",
       " '킨': 713,\n",
       " '롬': 714,\n",
       " '촐': 715,\n",
       " '챔': 716,\n",
       " '썼': 717,\n",
       " '삐': 718,\n",
       " '씌': 719,\n",
       " '옳': 720,\n",
       " '만': 721,\n",
       " '끊': 722,\n",
       " '듬': 723,\n",
       " '아': 724,\n",
       " '푹': 725,\n",
       " '익': 726,\n",
       " '둥': 727,\n",
       " '찝': 728,\n",
       " '종': 729,\n",
       " 'q': 730,\n",
       " '쌈': 731,\n",
       " '인': 732,\n",
       " '놀': 733,\n",
       " '넋': 734,\n",
       " '권': 735,\n",
       " '…': 736,\n",
       " '릎': 737,\n",
       " '덤': 738,\n",
       " '쟁': 739,\n",
       " '샘': 740,\n",
       " '잎': 741,\n",
       " '넴': 742,\n",
       " '울': 743,\n",
       " '린': 744,\n",
       " '흉': 745,\n",
       " '곶': 746,\n",
       " '커': 747,\n",
       " '뚫': 748,\n",
       " '땄': 749,\n",
       " '적': 750,\n",
       " '핥': 751,\n",
       " '김': 752,\n",
       " '머': 753,\n",
       " '뮤': 754,\n",
       " '랐': 755,\n",
       " '러': 756,\n",
       " '닮': 757,\n",
       " '클': 758,\n",
       " '컵': 759,\n",
       " '껐': 760,\n",
       " '슴': 761,\n",
       " '횡': 762,\n",
       " '얽': 763,\n",
       " '쫓': 764,\n",
       " '첩': 765,\n",
       " '건': 766,\n",
       " '훨': 767,\n",
       " '뷔': 768,\n",
       " '뻥': 769,\n",
       " '농': 770,\n",
       " '춰': 771,\n",
       " '칫': 772,\n",
       " '랖': 773,\n",
       " '앵': 774,\n",
       " '렐': 775,\n",
       " '룻': 776,\n",
       " '8': 777,\n",
       " '들': 778,\n",
       " '옆': 779,\n",
       " '푯': 780,\n",
       " '꾼': 781,\n",
       " '셨': 782,\n",
       " '큽': 783,\n",
       " '윈': 784,\n",
       " '웰': 785,\n",
       " '녹': 786,\n",
       " '광': 787,\n",
       " '촉': 788,\n",
       " '랩': 789,\n",
       " '멸': 790,\n",
       " '벌': 791,\n",
       " '퉁': 792,\n",
       " '궂': 793,\n",
       " '휴': 794,\n",
       " '싫': 795,\n",
       " '젯': 796,\n",
       " '웍': 797,\n",
       " '우': 798,\n",
       " '뜸': 799,\n",
       " '돔': 800,\n",
       " '혐': 801,\n",
       " '서': 802,\n",
       " '끼': 803,\n",
       " '콜': 804,\n",
       " '북': 805,\n",
       " '쁠': 806,\n",
       " '뻘': 807,\n",
       " '촬': 808,\n",
       " '계': 809,\n",
       " '걷': 810,\n",
       " '통': 811,\n",
       " '온': 812,\n",
       " '구': 813,\n",
       " '샹': 814,\n",
       " '자': 815,\n",
       " '점': 816,\n",
       " '륙': 817,\n",
       " '톱': 818,\n",
       " '헀': 819,\n",
       " '뵈': 820,\n",
       " '염': 821,\n",
       " '둑': 822,\n",
       " '볼': 823,\n",
       " '얌': 824,\n",
       " '낮': 825,\n",
       " '뮬': 826,\n",
       " '퀄': 827,\n",
       " '믄': 828,\n",
       " '셈': 829,\n",
       " '샷': 830,\n",
       " '한': 831,\n",
       " '교': 832,\n",
       " '잠': 833,\n",
       " '돋': 834,\n",
       " '꽝': 835,\n",
       " '맣': 836,\n",
       " '짜': 837,\n",
       " '냄': 838,\n",
       " '퀴': 839,\n",
       " '믈': 840,\n",
       " '큼': 841,\n",
       " '엮': 842,\n",
       " '돠': 843,\n",
       " '캣': 844,\n",
       " '피': 845,\n",
       " '쟎': 846,\n",
       " '보': 847,\n",
       " '많': 848,\n",
       " '습': 849,\n",
       " '성': 850,\n",
       " '걍': 851,\n",
       " '뜰': 852,\n",
       " '증': 853,\n",
       " '큰': 854,\n",
       " '찜': 855,\n",
       " '삼': 856,\n",
       " '녁': 857,\n",
       " '변': 858,\n",
       " '붇': 859,\n",
       " '짖': 860,\n",
       " '긍': 861,\n",
       " '칩': 862,\n",
       " '챠': 863,\n",
       " '의': 864,\n",
       " '3': 865,\n",
       " '젋': 866,\n",
       " '득': 867,\n",
       " '쌩': 868,\n",
       " 'g': 869,\n",
       " '열': 870,\n",
       " '냥': 871,\n",
       " '쉐': 872,\n",
       " '젓': 873,\n",
       " '쭤': 874,\n",
       " '탔': 875,\n",
       " '휙': 876,\n",
       " '갭': 877,\n",
       " 'h': 878,\n",
       " '즉': 879,\n",
       " '홈': 880,\n",
       " '았': 881,\n",
       " '분': 882,\n",
       " '냉': 883,\n",
       " '文': 884,\n",
       " '견': 885,\n",
       " '멕': 886,\n",
       " '뜻': 887,\n",
       " '겐': 888,\n",
       " '멍': 889,\n",
       " '숴': 890,\n",
       " '향': 891,\n",
       " '씽': 892,\n",
       " '륭': 893,\n",
       " '잡': 894,\n",
       " '팬': 895,\n",
       " '퍠': 896,\n",
       " '낭': 897,\n",
       " '텀': 898,\n",
       " '끈': 899,\n",
       " '샴': 900,\n",
       " '느': 901,\n",
       " '댑': 902,\n",
       " '넉': 903,\n",
       " '地': 904,\n",
       " '겼': 905,\n",
       " '쑥': 906,\n",
       " '푼': 907,\n",
       " '줬': 908,\n",
       " '딧': 909,\n",
       " '카': 910,\n",
       " '읍': 911,\n",
       " '끝': 912,\n",
       " '코': 913,\n",
       " '턱': 914,\n",
       " '욕': 915,\n",
       " '걔': 916,\n",
       " '덮': 917,\n",
       " '뉘': 918,\n",
       " '핫': 919,\n",
       " '물': 920,\n",
       " '걀': 921,\n",
       " '봤': 922,\n",
       " '팡': 923,\n",
       " '식': 924,\n",
       " '뤄': 925,\n",
       " '맵': 926,\n",
       " '근': 927,\n",
       " '릴': 928,\n",
       " '탓': 929,\n",
       " '엊': 930,\n",
       " '꼈': 931,\n",
       " '헬': 932,\n",
       " '썪': 933,\n",
       " '랗': 934,\n",
       " '나': 935,\n",
       " '왠': 936,\n",
       " '퇴': 937,\n",
       " '렀': 938,\n",
       " '꿍': 939,\n",
       " '킬': 940,\n",
       " '1': 941,\n",
       " '빗': 942,\n",
       " '위': 943,\n",
       " '댈': 944,\n",
       " '담': 945,\n",
       " '짭': 946,\n",
       " '멩': 947,\n",
       " '엡': 948,\n",
       " '문': 949,\n",
       " '꼭': 950,\n",
       " '강': 951,\n",
       " '턴': 952,\n",
       " '털': 953,\n",
       " '세': 954,\n",
       " '립': 955,\n",
       " '윽': 956,\n",
       " '찐': 957,\n",
       " '렬': 958,\n",
       " '옜': 959,\n",
       " '덜': 960,\n",
       " '반': 961,\n",
       " '찰': 962,\n",
       " '퓨': 963,\n",
       " '룡': 964,\n",
       " '듀': 965,\n",
       " '잿': 966,\n",
       " '끗': 967,\n",
       " '밥': 968,\n",
       " '즌': 969,\n",
       " '무': 970,\n",
       " '버': 971,\n",
       " '뻑': 972,\n",
       " '땃': 973,\n",
       " '놨': 974,\n",
       " '뻤': 975,\n",
       " '댔': 976,\n",
       " '림': 977,\n",
       " '롱': 978,\n",
       " '뀌': 979,\n",
       " '슷': 980,\n",
       " '뜹': 981,\n",
       " '봅': 982,\n",
       " '겔': 983,\n",
       " '옯': 984,\n",
       " '윾': 985,\n",
       " '킷': 986,\n",
       " '띠': 987,\n",
       " '뒷': 988,\n",
       " '릿': 989,\n",
       " '팝': 990,\n",
       " '춥': 991,\n",
       " '캡': 992,\n",
       " '쏠': 993,\n",
       " '듐': 994,\n",
       " '틱': 995,\n",
       " '개': 996,\n",
       " '송': 997,\n",
       " '꿨': 998,\n",
       " '닙': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_decomposer.ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "outstanding-masters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in title vocabulary: 1610\n",
      "Unique tokens in label vocabulary: 9\n",
      "number of training data : 45678\n",
      "number of valid data : 9107\n",
      "number of test data : 10\n"
     ]
    }
   ],
   "source": [
    "class CLSDataset:\n",
    "    def __init__(self, elmo_dataset, filepath, batch_size, max_length, device):\n",
    "        self.elmo_dataset = elmo_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.title = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                                init_token='<sos>',\n",
    "                                eos_token='<eos>',\n",
    "                                pad_token='<pad>',\n",
    "                                lower=True,\n",
    "                                batch_first=True,\n",
    "                                include_lengths=False, \n",
    "                                fix_length=max_length)\n",
    "        \n",
    "        self.label = data.Field(lower=True,\n",
    "                                batch_first=True)\n",
    "        \n",
    "        fields = [('label', self.label), ('title', self.title)]\n",
    "        self.train_data, self.valid_data, self.test_data = data.TabularDataset.splits(path=filepath,\n",
    "                                                                                      train='train_tokenized.ynat',\n",
    "                                                                                      validation='val_tokenized.ynat',\n",
    "                                                                                      test='test_tokenized.ynat',\n",
    "                                                                                      format='tsv',\n",
    "                                                                                      fields=fields)\n",
    "        \n",
    "        self.build_vocab()\n",
    "        print('number of training data : {}'.format(len(self.train_data)))\n",
    "        print('number of valid data : {}'.format(len(self.valid_data)))\n",
    "        print('number of test data : {}'.format(len(self.test_data)))\n",
    "        \n",
    "        self.train_iterator, self.valid_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (self.train_data, self.valid_data, self.test_data), sort=True, sort_within_batch=True,\n",
    "            batch_size=self.batch_size, device=self.device, sort_key=lambda x: len(x.title))\n",
    "        \n",
    "    def build_vocab(self):\n",
    "#         self.title.build_vocab(self.train_data, min_freq=1)\n",
    "        self.title.vocab = self.elmo_dataset.SRC.vocab\n",
    "        self.label.build_vocab(self.train_data, min_freq=1)\n",
    "        \n",
    "        print(f\"Unique tokens in title vocabulary: {len(self.title.vocab)}\")\n",
    "        print(f\"Unique tokens in label vocabulary: {len(self.label.vocab)}\")\n",
    "        \n",
    "cls_dataset = CLSDataset(elmo_dataset, filepath=\"data\", batch_size=BATCH_SIZE, max_length=MAX_WORD_LENGTH_IN_SENT, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "expected-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, pad_idx, dropout=0.2):\n",
    "        super(CNN1d, self).__init__()   \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels  = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size  = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):       \n",
    "        embedded = self.embedding(src)\n",
    "        batch_size, word_len, char_len, emb_dim = embedded.size()\n",
    "        \n",
    "        # [batch * word_len, char_len, emb_dim]\n",
    "        embedded = embedded.reshape(-1, char_len, emb_dim)  \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat    = self.dropout(torch.cat(pooled, dim=1))\n",
    "        \n",
    "        output = self.fc(cat)\n",
    "        output = output.reshape(batch_size, word_len, -1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, n_layers, f):\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(n_layers)])\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in range(self.n_layers):\n",
    "            gate = F.sigmoid(self.gate[layer](x))\n",
    "\n",
    "            nonlinear = self.f(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "\n",
    "        return x\n",
    "    \n",
    "class ELMO_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, n_layers=2, bidirectional=True, dropout_rate=0.2):\n",
    "        super(ELMO_Embedding, self).__init__()\n",
    "\n",
    "        n_filters = 100\n",
    "        filter_sizes = [3, 4, 5]\n",
    "\n",
    "        self.embedding = CNN1d(vocab_size, embedding_dim, n_filters, filter_sizes, embedding_dim, pad_idx, dropout_rate)\n",
    "        self.highway   = Highway(size=embedding_dim, n_layers=1, f=F.relu)\n",
    "        self.rnn       = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional, batch_first=True)        \n",
    "        self.fc_out    = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, max_word_len=20):\n",
    "        hiddens = []\n",
    "        embeddings = []\n",
    "        steps = max_word_len\n",
    "        \n",
    "        for step in range(steps):\n",
    "            inputs = src[:, i, :].unsqueeze(1)\n",
    "            embedding = self.highway(self.embedding(inputs))\n",
    "            output, (hidden, state) = self.rnn(embedding)\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "            hiddens.append(hidden)\n",
    "\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        return embeddings, hiddens\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMO_Task(nn.Module):\n",
    "    def __init__(self, vocab_size, LM_embedding_dim, LM_hidden_dim, LM_output_dim, TASK_embedding_dim, TASK_hidden_dim, pad_idx):\n",
    "        super(ELMO_Task, self).__init__()\n",
    "        # eval 을 사용하고 task 관련 부분을 다른 subclass로 나누면 해결할 수 있을 것으로 보임\n",
    "        # save - load 는 모델의 구조까지 다 관리 가능함 / 굳이 load_state_dict 쓰지 않아도 됨\n",
    "        self.elmo_embedding = ELMO_Embedding(vocab_size, LM_embedding_dim, LM_hidden_dim, LM_output_dim, pad_idx, dropout_rate=0)\n",
    "        self.elmo_embedding.load_state_dict(torch.load('weights/ELMO-LM_best.pt'))\n",
    "\n",
    "        for param in self.elmo_embedding.features.parameters():\n",
    "            param.require_grad = False\n",
    "        \n",
    "        self.layer_coef = nn.Parameter(torch.ones(1, 3 , require_grad=True))\n",
    "        self.scale_coef = nn.Parameter(torch.ones(1, require_grad=True))\n",
    "        self.softmax    = nn.Softmax(dim=0)\n",
    "        self.sigmoid    = nn.sigmoid()\n",
    "        \n",
    "        self.rnn = nn.GRU(TASK_embedding_dim, TASK_hidden_dim, n_layers=2, batch_first=True)\n",
    "#         self.fc  = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src_embedding, src_hiddens = self.elmo_embedding(src)\n",
    "        elmo_representation = torch.cat([src_embedding, src_hiddens], dim=-1)\n",
    "        representation = torch.matmul(self.softmax(self.layer_coef), elmo_representation)\n",
    "        # sigmoid check\n",
    "        representation *= self.sigmoid(self.scale_coef)\n",
    "        representation = torch.sum(representation)\n",
    "        \n",
    "        pred = self.fc(self.rnn(representation))\n",
    "\n",
    "        return pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cutting-separation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  512,   256,   256,   256,   256,   256],\n",
      "        [11275,   256,   256,   256,   256,   256],\n",
      "        [66522,   256,   256,   256,   256,   256],\n",
      "        [49775,   256,   256,   256,   256,   256],\n",
      "        [ 9772,   256,   256,   256,   256,   256],\n",
      "        [  572,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256],\n",
      "        [  256,   256,   256,   256,   256,   256]])\n"
     ]
    }
   ],
   "source": [
    "for batch in cls_dataset.train_iterator: \n",
    "    title = character_decomposer.decompose(batch.title)\n",
    "    \n",
    "    print(sum([t for t in title]))\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "american-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CLSDataset at 0x7f3a61e2d7d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-twenty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
