{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f028cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "import torch\n",
    "from torchtext import data, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4bdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_source_and_target(lines, split_cond):\n",
    "    src = []\n",
    "    trg = []\n",
    "\n",
    "    for line in lines:\n",
    "        src.append(' '.join(line[:-1]) + '\\n')\n",
    "        trg.append(' '.join(line[1:]) + '\\n')\n",
    "    \n",
    "    write_txt(split_cond + \".src\", src)\n",
    "    write_txt(split_cond + \".trg\", trg)\n",
    "    \n",
    "def write_txt(fname, lines, fpath=\"data\"):\n",
    "    if fpath is not None:\n",
    "        with open(os.path.join(fpath, fname), \"w\") as f:\n",
    "            f.writelines(lines)\n",
    "    elif fpath is None:\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81829fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/petitions_splited_mecab.txt\", \"r\") as f:\n",
    "     corpus = f.readlines()\n",
    "\n",
    "corpus = list(map(lambda x: str(x).replace(\"\\n\", \"\"), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573ac543",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines, test_lines = train_test_split(corpus, test_size=0.05, random_state=1234)\n",
    "train_lines, valid_lines = train_test_split(train_lines, test_size=1/19, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5582b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_source_and_target(train_lines, \"train\")\n",
    "generate_source_and_target(valid_lines, \"val\")\n",
    "generate_source_and_target(test_lines, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1774090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "MAX_WORD_LENGTH_IN_SENT = 25\n",
    "MAX_CHAR_LENGTH_IN_WORD = 6\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44b81395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source vocabulary: 1610\n",
      "Unique tokens in target vocabulary: 1602\n",
      "number of training data : 205654\n",
      "number of valid data : 11426\n",
      "number of test data : 11426\n"
     ]
    }
   ],
   "source": [
    "class ELMODataset:\n",
    "    def __init__(self, filepath, batch_size, max_length, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.SRC = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                              init_token='<sos>',\n",
    "                              eos_token='<eos>',\n",
    "                              pad_token='<pad>',\n",
    "                              lower=True,\n",
    "                              batch_first=True,\n",
    "                              include_lengths=False,\n",
    "                              fix_length=max_length)\n",
    "        \n",
    "        self.TRG = data.Field(tokenize=lambda x: x.split(' '),\n",
    "                              init_token='<sos>',\n",
    "                              eos_token='<eos>',\n",
    "                              pad_token='<pad>',\n",
    "                              lower=True,\n",
    "                              batch_first=True,\n",
    "                              fix_length=max_length)\n",
    "\n",
    "        self.train_data, self.valid_data, self.test_data = \\\n",
    "            datasets.TranslationDataset.splits(path=filepath, exts=('.src', '.trg'),\n",
    "                                               fields=(self.SRC, self.TRG))\n",
    "\n",
    "        self.build_vocab()\n",
    "\n",
    "        print('number of training data : {}'.format(len(self.train_data)))\n",
    "        print('number of valid data : {}'.format(len(self.valid_data)))\n",
    "        print('number of test data : {}'.format(len(self.test_data)))\n",
    "\n",
    "        self.train_iterator, self.valid_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (self.train_data, self.valid_data, self.test_data), sort=True, sort_within_batch=True,\n",
    "            batch_size=self.batch_size, device=self.device)\n",
    " \n",
    "    def build_vocab(self, min_freq=5):\n",
    "        self.SRC.build_vocab(self.train_data, min_freq=min_freq)\n",
    "        self.TRG.build_vocab(self.train_data, min_freq=min_freq)\n",
    "        \n",
    "        print(f\"Unique tokens in source vocabulary: {len(self.SRC.vocab)}\")\n",
    "        print(f\"Unique tokens in target vocabulary: {len(self.TRG.vocab)}\")\n",
    "\n",
    "elmo_dataset = ELMODataset(filepath=\"data\", batch_size=BATCH_SIZE, max_length=MAX_WORD_LENGTH_IN_SENT, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83bec217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterDecomposer:\n",
    "    def __init__(self, elmo_dataset, max_word_in_sent, max_char_in_word, special_token_idx=[0, 1, 2, 3]):\n",
    "        self.elmo_dataset = elmo_dataset\n",
    "        self.max_word_in_sent = max_word_in_sent\n",
    "        self.max_char_in_word = max_char_in_word\n",
    "        self.special_token_idx = special_token_idx\n",
    "        \n",
    "        self.build_char_vocab()\n",
    "        \n",
    "    def build_char_vocab(self):\n",
    "        char_vocab = set([char for word in self.elmo_dataset.SRC.vocab.itos for char in word])\n",
    "        self.ctoi = {}\n",
    "        self.itoc = {}\n",
    "        \n",
    "        for idx, char in enumerate(char_vocab):\n",
    "            self.ctoi[char] = idx\n",
    "            self.itoc[idx]  = char\n",
    "            \n",
    "    def decompose(self, src):\n",
    "        # pad token이 1로 되어 있음\n",
    "        batch_char_embedding = np.ones((src.shape[0], self.max_word_in_sent, self.max_char_in_word)).astype(int)\n",
    "        \n",
    "        for batch_order_idx, sent in enumerate(src):\n",
    "            for word_order_idx, s in enumerate(sent):\n",
    "                if word_order_idx < self.max_word_in_sent - 1:\n",
    "                    if s in self.special_token_idx:\n",
    "                        batch_char_embedding[batch_order_idx, word_order_idx, 0] = s\n",
    "                        pass\n",
    "                    elif s not in self.special_token_idx:\n",
    "                        for char_order_idx, char in enumerate(self.elmo_dataset.SRC.vocab.itos[s]):\n",
    "                            if char_order_idx < self.max_char_in_word - 1:\n",
    "                                batch_char_embedding[batch_order_idx, word_order_idx, char_order_idx] = self.ctoi[char]\n",
    "                                                             \n",
    "        return torch.LongTensor(batch_char_embedding)\n",
    "    \n",
    "character_decomposer = CharacterDecomposer(elmo_dataset, max_word_in_sent=MAX_WORD_LENGTH_IN_SENT, max_char_in_word=MAX_CHAR_LENGTH_IN_WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a22a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606 ms ± 5.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for batch in elmo_dataset.train_iterator:\n",
    "    src = batch.src\n",
    "    character_decomposer.decompose(src)\n",
    "#     print(character_decomposer.decompose(src))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
